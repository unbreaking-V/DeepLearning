{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange\n",
    "from torchvision.ops.stochastic_depth import StochasticDepth # Add stochastic depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from torchvision.ops.stochastic_depth import StochasticDepth \n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.ops import FeaturePyramidNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Partition + Linear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEmbedding(nn.Module):\n",
    "\n",
    "  \"\"\"\n",
    "  input shape -> (b,c,h,w)\n",
    "  output shape -> (b, h/4 , w/4, C)\n",
    "\n",
    "  Where:\n",
    "\n",
    "  b - batch size\n",
    "  h - height of the image\n",
    "  w - width of the image\n",
    "  C - number of channels\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, patch_size = 4, C = 96):\n",
    "      super().__init__()\n",
    "      self.linear_embedding = nn.Conv2d(3,C, kernel_size=patch_size, stride=patch_size)\n",
    "      self.layer_norm = nn.LayerNorm(C)\n",
    "      self.relu = nn.ReLU() # activation function (not present in the torchvision model)\n",
    "\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.linear_embedding(x) # image partitioning into patches\n",
    "    x = rearrange(x, 'b c h w -> b h w c')  # change the shape of the tensor\n",
    "    x = self.layer_norm(x) \n",
    "    x = self.relu(x) # activation function (not present in the torchvision model)\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Merging Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "  \"\"\"\n",
    "  Reduces tokens by a factor of 4 (2x2 patches) and doubles embedding dimension.\n",
    "\n",
    "\n",
    "  input shape -> (b h w c)\n",
    "  output shape -> (b h/2 w/2 C*2)\n",
    "\n",
    "  Where:\n",
    "\n",
    "  b - batch size\n",
    "  c - number of channels\n",
    "  h - height of the image\n",
    "  w - width of the image\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, C) -> None:\n",
    "     super().__init__()\n",
    "     self.linear_layer = nn.Linear(C*4, C*2) # Doubles the embedding dimension\n",
    "     self.layer_norm = nn.LayerNorm(2 * C) # Layer normalization\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = rearrange(x, 'b (h ph) (w pw) c -> b h w (ph pw c)', ph=2, pw=2) # Merge patches and double the embedding dimension\n",
    "    x = self.linear_layer(x) \n",
    "    x = self.layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifted Window Attention Mechanism\n",
    "\n",
    "In order for the model to effectively process input data, the feature maps are padded before applying the attention mechanism. This is necessary to ensure that the feature map dimensions are divisible by the window size, as the model operates by partitioning the input into non-overlapping windows, and padding ensures that all elements are considered. After padding, the feature map is divided into windows, allowing attention to be computed within each window. This step prevents potential distortions that could occur if the window sizes were not uniform and ensures stable operation of the model.\n",
    "\n",
    "Next, to address the issue of a few pixel pairs dominating the self-attention mechanism when the model size increases, the method of **scaled cosine attention** is proposed. Instead of computing attention using the standard dot product of the query and key vectors, attention is computed using the cosine similarity between these vectors. This similarity is then scaled by a learnable parameter $\\tau$, which is different for each head and layer. This scaling helps control the magnitude of the attention values, ensuring the stability of the model, especially in larger versions. The formula for computing the similarity between pixels $i$ and $j$ is as follows:\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{Sim}(q_i, k_j) = \\frac{\\cos(q_i, k_j)}{\\tau} + B_{ij}\n",
    "\n",
    "$$\n",
    "\n",
    "where $q_i$ and $k_j$ are the query and key vectors, and $\\tau$ is the learnable scaling factor. An important addition is $B_{ij}$, which represents the relative position bias. This bias is added to account for the relative positions of pixels within the window, allowing the model to better handle varying window sizes.\n",
    "\n",
    "Thus, the relative position bias $B_{ij}$ plays a key role in improving the attention computation. Unlike the fixed positional encoding used in traditional transformers, this bias is computed dynamically for each pair of pixels based on their relative positions within the window. Introducing **relative position bias** into the attention formula enables the model to be more adaptive when working with images of different resolutions and window sizes, improving its performance and stability.\n",
    "\n",
    "![image](../images/Pasted%20image%20(2).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedWindowMSA(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size=8, mask=False, attention_dropout=0.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mask = mask # mask (True/False)\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n",
    "\n",
    "        self.relative_embeddings = RelativeEmbeddingsV2(window_size, num_heads)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        B, H, W, C = input.shape\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(input, (0, 0, 0, pad_r, 0, pad_b))\n",
    "        _, pad_H, pad_W, _ = x.shape\n",
    "       \n",
    "        # Cyclic shift\n",
    "        if self.mask:\n",
    "            x = torch.roll(x, (-self.window_size//2, -self.window_size//2), dims=(1,2))\n",
    "\n",
    "        # Partition windows\n",
    "        num_windows = (pad_H //self.window_size) * (pad_W // self.window_size)\n",
    "        x = rearrange(\n",
    "                    x, \n",
    "                    'b (h w_h) (w w_w) c -> (b h w) (w_h w_w) c', \n",
    "                    w_h=self.window_size, w_w=self.window_size\n",
    "                )\n",
    "\n",
    "        # QKV computation\n",
    "        qkv = F.linear(x, self.qkv.weight)\n",
    "        qkv = qkv.reshape(x.size(0), x.size(1), 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # Scaled dot-product attention with logit scaling \n",
    "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)  # cos(qi, kj), cos - normalizetion\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=math.log(100.0)).exp() # tau \n",
    "        attn = attn * logit_scale  # cos(qi, kj) / tau\n",
    "\n",
    "        # Add relative position bias\n",
    "        relative_position_bias = self.relative_embeddings()\n",
    "        attn = attn + relative_position_bias  # cos(qi, kj) / tau + b_ij\n",
    "       \n",
    "        if self.mask:\n",
    "            # Create attention mask\n",
    "            attn_mask = torch.zeros((pad_H, pad_W), device=x.device)\n",
    "            \n",
    "            # Generate coordinates for the mask\n",
    "            for i in range(0, pad_H, self.window_size):\n",
    "                for j in range(0, pad_W, self.window_size):\n",
    "                    attn_mask[i:i + self.window_size, j:j + self.window_size] += 1\n",
    "            \n",
    "            # Create mask for each window\n",
    "            attn_mask = rearrange(\n",
    "                attn_mask, \n",
    "                '(h winh) (w winw) -> (h w) (winh winw)', \n",
    "                winh=self.window_size, \n",
    "                winw=self.window_size\n",
    "            )\n",
    "\n",
    "            # Create mask for each window\n",
    "            attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)  # Shape: (num_windows, window_size^2, window_size^2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float('-inf')).masked_fill(attn_mask == 0, 0.0)\n",
    "\n",
    "            # Add a dimension for num_heads\n",
    "            attn_mask = attn_mask.unsqueeze(1)  # Shape: (num_windows, 1, window_size^2, window_size^2)\n",
    "\n",
    "            # Broadcast over batch and num_heads\n",
    "            attn = attn.view(-1, num_windows, self.num_heads, x.size(1), x.size(1))\n",
    "            attn = attn + attn_mask.unsqueeze(0)  # Broadcasting over batch and num_heads\n",
    "            attn = attn.view(-1, self.num_heads, x.size(1), x.size(1))\n",
    "\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "\n",
    "        # Attention output\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, -1, C) # attn = softmax(cos(qi, kj) / tau + b_ij) @ v\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_dropout(x)\n",
    "\n",
    "        # Reverse cyclic shift\n",
    "        x = rearrange(\n",
    "            x, \n",
    "            'b (h ws1 w ws2) c -> b (h ws1) (w ws2) c', \n",
    "            ws1=self.window_size, \n",
    "            ws2=self.window_size,\n",
    "            h = pad_H // self.window_size,\n",
    "            w = pad_W // self.window_size\n",
    "        )\n",
    "        if self.mask: \n",
    "            x = torch.roll(x, (self.window_size//2, self.window_size//2), (1,2))\n",
    "\n",
    "        #unpad features\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Position Embeddings \n",
    "\n",
    "In the paper, **relative position bias** is introduced as a mechanism to handle varying window resolutions effectively. The traditional approach to relative position bias, which directly optimizes parameterized biases, can become problematic when transferring models across different window sizes. To address this, the paper proposes a **log-spaced continuous position bias approach** that can be smoothly transferred to fine-tuning tasks with arbitrary window sizes.\n",
    "\n",
    "The main idea is to use a **meta network**, denoted as $G(\\Delta x, \\Delta y)$, which generates bias values for relative coordinates. Instead of directly optimizing biases, this network computes biases based on the relative positions of elements in the window. In this way, the generated biases can be adapted to different window sizes without requiring retraining. The continuous position bias approach allows the bias values to be precomputed and stored as model parameters, making inference efficient and consistent with the original parameterized bias approach.\n",
    "\n",
    "Moreover, the paper addresses the challenge of **extrapolating biases** when transferring models across significantly different window sizes. The original approach used linearly spaced coordinates, which led to large extrapolation ratios. To mitigate this, the paper introduces **log-spaced coordinates**, which reduce the required extrapolation when transferring biases across window resolutions. The transformation from linear coordinates $\\Delta x, \\Delta y$ to log-spaced coordinates $\\Delta cx$, $\\Delta cy$ is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\Delta cx = \\text{sign}(x) \\cdot \\log(1 + | \\Delta x |),\n",
    "\\quad \\Delta cy = \\text{sign}(y) \\cdot \\log(1 + | \\Delta y |).\n",
    "$$\n",
    "\n",
    "\n",
    "This log transformation helps to keep the extrapolation ratio smaller, which in turn makes the transfer of relative position biases more stable and effective across different window sizes.\n",
    "\n",
    "![image](../images/Pasted%20image%20(3).png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeEmbeddingsV2(nn.Module):\n",
    "    def __init__(self, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size  # Size of the window (e.g., 8x8 or 16x16)\n",
    "        self.num_heads = num_heads  # Number of attention heads\n",
    "\n",
    "        # Define the MLP used for continuous position bias\n",
    "        self.cpb_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 512, bias=True),  # First linear layer (maps 2D position to 512 features)\n",
    "            nn.ReLU(inplace=True),         # ReLU activation\n",
    "            nn.Linear(512, num_heads, bias=False),  # Second linear layer (maps 512 features to num_heads)\n",
    "        )\n",
    "\n",
    "        # Initialize relative coordinates and relative position index\n",
    "        self.define_relative_coords()\n",
    "        self.define_relative_position_index()\n",
    "\n",
    "    def define_relative_coords(self):\n",
    "        \"\"\"\n",
    "        This method defines the relative coordinates for each pixel within the window. \n",
    "        It creates a grid of relative coordinates and applies log transformation.\n",
    "        \"\"\"\n",
    "        # Generate range of relative coordinates along height and width (e.g., -7 to 7 for 8x8 window)\n",
    "        relative_coords_h = torch.arange(-(self.window_size - 1), self.window_size, dtype=torch.float32)\n",
    "        relative_coords_w = torch.arange(-(self.window_size - 1), self.window_size, dtype=torch.float32)\n",
    "\n",
    "        # Create a meshgrid of relative coordinates for height and width\n",
    "        relative_coords_table = torch.stack(torch.meshgrid(relative_coords_h, relative_coords_w, indexing=\"ij\"))\n",
    "\n",
    "        # Permute to match the required shape (2D, 2D, 2 -> 1, 2, 2, 2)\n",
    "        relative_coords_table = relative_coords_table.permute(1, 2, 0).contiguous().unsqueeze(0)\n",
    "\n",
    "        # Normalize the relative coordinates to the range [-1, 1] for both axes\n",
    "        relative_coords_table[..., 0] /= (self.window_size - 1)\n",
    "        relative_coords_table[..., 1] /= (self.window_size - 1)\n",
    "\n",
    "        # Apply scaling to the coordinates (multiply by 8)\n",
    "        relative_coords_table *= 8\n",
    "\n",
    "        # Apply the log transformation to the relative coordinates\n",
    "        relative_coords_table = (\n",
    "            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / 3.0\n",
    "        )\n",
    "        \n",
    "        # Register the relative coordinates table as a buffer to be used during training\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n",
    "\n",
    "    def define_relative_position_index(self):\n",
    "        \"\"\"\n",
    "        This method defines the relative position index for each pixel pair in the window.\n",
    "        It calculates the differences in positions and generates a unique index for each relative position.\n",
    "        \"\"\"\n",
    "        # Generate coordinates for the height and width of the window\n",
    "        coords_h = torch.arange(self.window_size)\n",
    "        coords_w = torch.arange(self.window_size)\n",
    "\n",
    "        # Create a meshgrid for all the coordinates\n",
    "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
    "\n",
    "        # Flatten the coordinates into a 2D array\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "\n",
    "        # Calculate the relative position by subtracting each pair of coordinates\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "\n",
    "        # Shift the coordinates to ensure positive indices\n",
    "        relative_coords[:, :, 0] += self.window_size - 1\n",
    "        relative_coords[:, :, 1] += self.window_size - 1\n",
    "\n",
    "        # Scale the coordinates to a larger range (for uniqueness)\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "\n",
    "        # Sum the two coordinate differences to get a unique index\n",
    "        relative_position_index = relative_coords.sum(-1).flatten()\n",
    "\n",
    "        # Register the relative position index as a buffer to be used during training\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        This method computes the relative position bias using the pre-defined meta network (MLP) and relative position table.\n",
    "        \"\"\"\n",
    "        # Use the relative position index and the relative coordinates table to compute the bias\n",
    "        relative_position_bias = F.embedding(\n",
    "            self.relative_position_index,  # Look up bias values from the relative position index\n",
    "            self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads),  # Apply MLP to relative coords table\n",
    "        )\n",
    "\n",
    "        # Reshape the bias values to match the shape of the attention logits (window_size * window_size, window_size * window_size, num_heads)\n",
    "        relative_position_bias = relative_position_bias.view(\n",
    "            self.window_size * self.window_size, self.window_size * self.window_size, self.num_heads\n",
    "        )\n",
    "\n",
    "        # Permute the bias to match the attention mechanism (num_heads, window_size * window_size, window_size * window_size)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().unsqueeze(0)\n",
    "\n",
    "        # Apply a sigmoid activation to the bias values to smooth them and scale by a factor of 16\n",
    "        return 16 * torch.sigmoid(relative_position_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer Block v2 \n",
    "\n",
    "The main difference of the Swin Transformer block of the second version is the change of the normalization order. The normalization layer was moved before the skip connection adder, which reduced the amplitude of activations and provided more stable and efficient learning.\n",
    "\n",
    "In addition, a stochastic drop path operation was added to the block to improve regularization. This is especially important for deep models and transformers, where this approach has been shown to perform better according to research.\n",
    "\n",
    "Initialization of weights and biases was also introduced, which promotes stable learning and accelerates model convergence due to correct distribution of initial parameters\n",
    "\n",
    "![image.png](/home/wladyka/Swin-Transformer/images/swin_transformer_block_v2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size, mask, sd_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.stochastic_depth = StochasticDepth(sd_prob, \"row\") # Stochastic Depth with 0.1 probability of dropping out a row for tiny version of Swin Transformer\n",
    "\n",
    "        self.WMSA = ShiftedWindowMSA(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=mask)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.1), # Default dropout probability is 0.0 in the torchvision implementation\n",
    "            nn.Linear(embed_dim*4, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Initialization of weights and biases (bias) in linear layers \n",
    "        for m in self.MLP:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight) # Xavier initialization for weights, which prevents the disappearance or explosion of gradients during training.\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6) # Set a small offset, to have a small impact in the initial stages of training.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Attention path with pre-normalization \n",
    "        res1 = x # Save input for the skip connection\n",
    "        x = self.stochastic_depth(self.layer_norm(self.WMSA(x))) # Attention block with LayerNorm and Stochastic Depth(more efficient than Dropout for training transformers)\n",
    "        x = res1 + x # Residual connection\n",
    "\n",
    "        # MLP path with pre-normalization\n",
    "        res2 = x  # Save intermediate result for skip connection\n",
    "        x = self.stochastic_depth(self.layer_norm(self.MLP(x))) # MLP block with LayerNorm and Dropout\n",
    "        x = res2 + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "    \n",
    "class AlternatingEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, sd_prob, window_size=8):\n",
    "        super().__init__()\n",
    "        self.WSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=False, sd_prob=sd_prob[0])\n",
    "        self.SWSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=True, sd_prob=sd_prob[1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.SWSA(self.WSA(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Swin-Transformer Class v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, depth=[2, 2, 6, 2], embed_dim=96, stochastic_depth_prob=0.2, window_size= 8):\n",
    "        super().__init__()\n",
    "        self.Embedding = SwinEmbedding()  # Embedding layer\n",
    "\n",
    "        # Calculate total number of blocks\n",
    "        total_stage_blocks = sum(depth)\n",
    "        stage_block_id = 0\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "\n",
    "        in_channels = embed_dim\n",
    "        for i_stage, num_blocks in enumerate(depth):\n",
    "            temp_sd_prob = []\n",
    "            for _ in range(num_blocks):\n",
    "                # Calculate probability for the current layer\n",
    "                sd_prob = stochastic_depth_prob * float(stage_block_id) / (total_stage_blocks - 1)\n",
    "                temp_sd_prob.append(sd_prob)\n",
    "                stage_block_id += 1\n",
    "\n",
    "            #Add alternating encoder blocks recording to the depth list divided by 2, because each block has 2 sub-blocks\n",
    "            sd_prob = [temp_sd_prob[i:i+2] for i in range(0, len(temp_sd_prob), 2)]\n",
    "            for _ in range(int(num_blocks / 2)):\n",
    "                num_heads = in_channels // 32\n",
    "                #print(f\"AlternatingEncoderBlock({in_channels}, {num_heads}, {sd_prob[0]})\") # Debug\n",
    "                self.stages.append(\n",
    "                    AlternatingEncoderBlock(in_channels, num_heads, sd_prob[0], window_size=window_size)\n",
    "                )\n",
    "                sd_prob.pop(0)\n",
    "                    \n",
    "            # Add patch merging layer if this is not the last stage\n",
    "            if i_stage < len(depth) - 1:\n",
    "                self.stages.append(PatchMerging(in_channels))\n",
    "                #print(f\"PatchMerging({in_channels})\") # Debug\n",
    "                in_channels *= 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x) \n",
    "        for stage in self.stages: \n",
    "            x = stage(x)\n",
    "            print(x.shape, stage._get_name()) # Debug\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 96]) AlternatingEncoderBlock\n",
      "torch.Size([1, 32, 32, 192]) PatchMerging\n",
      "torch.Size([1, 32, 32, 192]) AlternatingEncoderBlock\n",
      "torch.Size([1, 16, 16, 384]) PatchMerging\n",
      "torch.Size([1, 16, 16, 384]) AlternatingEncoderBlock\n",
      "torch.Size([1, 16, 16, 384]) AlternatingEncoderBlock\n",
      "torch.Size([1, 16, 16, 384]) AlternatingEncoderBlock\n",
      "torch.Size([1, 8, 8, 768]) PatchMerging\n",
      "torch.Size([1, 8, 8, 768]) AlternatingEncoderBlock\n",
      "Output shape: torch.Size([1, 8, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    x = torch.randn((1,3,256,256)).cuda()\n",
    "    model = SwinTransformer(depth=[2, 2, 6, 2], embed_dim=96, window_size=8).cuda()\n",
    "    print(f\"Output shape: {model(x).shape}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerMultiStage(nn.Module):\n",
    "    \"\"\"\n",
    "    Subclass (or replacement) of your SwinTransformer that returns\n",
    "    4 feature maps from each stage: C2, C3, C4, C5.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_swin):\n",
    "        super().__init__()\n",
    "        # Copy over the embedding\n",
    "        self.Embedding = base_swin.Embedding\n",
    "        # Copy over the entire 'stages' ModuleList\n",
    "        self.stages = base_swin.stages\n",
    "        # You already know embed_dim=96 for tiny model, but not strictly needed here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1) Patch embedding\n",
    "        x = self.Embedding(x)  # (B, 56*56, 96)\n",
    "\n",
    "        # -- Stage 1\n",
    "        x = self.stages[0](x)\n",
    "        c2 = x\n",
    "        x = self.stages[1](x)\n",
    "\n",
    "        # -- Stage 2\n",
    "        x = self.stages[2](x)\n",
    "        c3 = x\n",
    "        x = self.stages[3](x)\n",
    "\n",
    "        # -- Stage 3\n",
    "        x = self.stages[4](x)\n",
    "        x = self.stages[5](x)\n",
    "        x = self.stages[6](x)\n",
    "        c4 = x\n",
    "        x = self.stages[7](x)\n",
    "\n",
    "        # -- Stage 4\n",
    "        x = self.stages[8](x)\n",
    "        c5 = x\n",
    "        \n",
    "    \n",
    "        # Return all 4 feature maps, C2, C3, C4, C5 convert to (B, C, H, W)\n",
    "        stage_dict = {\n",
    "            \"c2\": rearrange(c2, 'B h w c -> B c h w'),\n",
    "            \"c3\": rearrange(c3, 'B h w c -> B c h w'),\n",
    "            \"c4\": rearrange(c4, 'B h w c -> B c h w'),\n",
    "            \"c5\": rearrange(c5, 'B h w c -> B c h w'),\n",
    "        }\n",
    "      \n",
    "\n",
    "        return stage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinFPNBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Runs the Swin stages -> returns c2..c5\n",
    "    2) Feeds them into a standard FeaturePyramidNetwork -> returns multi-scale feature maps\n",
    "    3) That final dict is what Mask R-CNN expects\n",
    "    \"\"\"\n",
    "    def __init__(self, swin_multistage: nn.Module):\n",
    "        super().__init__()\n",
    "        self.swin = swin_multistage\n",
    "        # Suppose we output 256 channels from FPN\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=[96, 192, 384, 768],  # channels in c2..c5\n",
    "            out_channels=256,\n",
    "            # extra_blocks=LastLevelMaxPool()  # optional\n",
    "        )\n",
    "        self.out_channels = 256  # FPN’s output channels per scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B,3,H,W)\n",
    "        # 1) Get raw stage features\n",
    "        features = self.swin(x)  # e.g. {\"c2\":(B,96,56,56), \"c3\":(B,192,28,28), \"c4\":(B,384,14,14), \"c5\":(B,768,7,7)}\n",
    "\n",
    "        # 2) Rename them to match FPN’s expected keys: \"0\", \"1\", \"2\", \"3\" or something\n",
    "        #    or you can pass them in as a dict with the same keys but then set in_channels_list accordingly\n",
    "        fpn_input = {\n",
    "        \"0\": features[\"c2\"],\n",
    "        \"1\": features[\"c3\"],\n",
    "        \"2\": features[\"c4\"],\n",
    "        \"3\": features[\"c5\"],\n",
    "         }\n",
    "\n",
    "        # 3) Run FPN\n",
    "        #    This returns a dict of feature maps at different scales (e.g. \"res2\", \"res3\", \"res4\", \"res5\")\n",
    "        #    each will have shape (B, 256, H_out, W_out)\n",
    "        out = self.fpn(fpn_input)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_swin_maskrcnn(num_classes=2):\n",
    "    base_swin = SwinTransformer(depth=[2, 2, 6, 2], embed_dim=96, window_size=7) # Tiny Swin Transformer\n",
    "    # Convert it to multi-stage\n",
    "    multi_stage_swin = SwinTransformerMultiStage(base_swin)\n",
    "    # Wrap in FPN\n",
    "    backbone = SwinFPNBackbone(multi_stage_swin)\n",
    "\n",
    "    # For multi-scale anchors\n",
    "    anchor_generator = anchor_generator = AnchorGenerator(\n",
    "    sizes=((32,), (64,), (128,), (256,)),  # 4 \"levels\"\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)*4     # or explicitly write 4 tuples\n",
    ")\n",
    "\n",
    "\n",
    "    transform = GeneralizedRCNNTransform(\n",
    "        min_size=224,\n",
    "        max_size=224,\n",
    "        image_mean=[0.0, 0.0, 0.0],\n",
    "        image_std=[1.0, 1.0, 1.0],\n",
    "    )\n",
    "\n",
    "    model = MaskRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_detections_per_img=100,\n",
    "        image_mean=None,\n",
    "        image_std=None,\n",
    "        transform=transform\n",
    "    )\n",
    "    # Force it in case older torchvision\n",
    "    model.transform = transform\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>), 'loss_mask': tensor(13.4425, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.6590, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0052, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "[{'boxes': tensor([[2.3617e+01, 1.1452e+00, 1.2960e+02, 3.1595e+01],\n",
      "        [6.6096e+01, 4.7692e-01, 1.1064e+02, 1.7900e+01],\n",
      "        [4.0986e+01, 1.7536e+02, 8.5376e+01, 1.9147e+02],\n",
      "        [6.9187e+01, 7.9406e-01, 1.0659e+02, 3.4072e+01],\n",
      "        [3.0238e+01, 3.8940e-01, 7.7502e+01, 1.6867e+01],\n",
      "        [8.4484e+01, 8.4817e-01, 1.3245e+02, 1.9481e+01],\n",
      "        [9.6718e+01, 2.1394e+02, 1.4480e+02, 2.2400e+02],\n",
      "        [1.7789e+02, 2.1278e+02, 2.2400e+02, 2.2400e+02],\n",
      "        [1.6205e+02, 2.1385e+02, 2.0793e+02, 2.2400e+02],\n",
      "        [1.6307e+02, 8.1699e-01, 2.0911e+02, 1.7677e+01],\n",
      "        [0.0000e+00, 1.6835e+02, 1.2681e+02, 2.2400e+02],\n",
      "        [4.8910e+01, 1.9448e+02, 8.0703e+01, 2.2400e+02],\n",
      "        [4.7048e+00, 2.1206e-01, 4.1390e+01, 3.8958e+01],\n",
      "        [4.2697e+01, 8.6530e-01, 7.9544e+01, 3.6854e+01],\n",
      "        [1.0676e+00, 1.8287e+01, 4.6258e+01, 3.5973e+01],\n",
      "        [0.0000e+00, 2.0282e+02, 6.3505e+01, 2.2400e+02],\n",
      "        [1.5906e+01, 1.4097e+02, 5.8220e+01, 1.5900e+02],\n",
      "        [2.0342e+02, 2.1480e+02, 2.2400e+02, 2.2400e+02],\n",
      "        [8.3450e+01, 2.0303e+02, 1.3208e+02, 2.2168e+02],\n",
      "        [4.8495e+01, 1.3678e+02, 9.2680e+01, 1.5451e+02],\n",
      "        [1.7720e+02, 1.3075e+00, 2.1388e+02, 3.4408e+01],\n",
      "        [1.0256e+02, 1.6690e+02, 1.4969e+02, 1.8211e+02],\n",
      "        [1.5760e+02, 1.9171e+02, 1.8917e+02, 2.2400e+02],\n",
      "        [1.2872e+01, 1.9529e+01, 5.7924e+01, 3.8849e+01],\n",
      "        [1.6487e+02, 1.9349e+02, 2.2400e+02, 2.2321e+02],\n",
      "        [1.3989e+02, 6.5789e-01, 1.8514e+02, 1.8383e+01],\n",
      "        [1.0282e+01, 1.4450e+00, 1.1409e+02, 5.0550e+01],\n",
      "        [9.6022e+01, 1.6692e+00, 1.9339e+02, 4.9688e+01],\n",
      "        [1.3031e+01, 3.0197e+00, 1.5597e+02, 8.1175e+01],\n",
      "        [8.2243e+01, 8.0391e+01, 1.0982e+02, 1.1518e+02],\n",
      "        [0.0000e+00, 2.3858e-02, 2.4450e+01, 3.0684e+01],\n",
      "        [4.8718e+01, 1.3473e+00, 1.6707e+02, 5.2704e+01],\n",
      "        [1.9942e+02, 2.0684e+02, 2.2400e+02, 2.2400e+02],\n",
      "        [1.7944e+02, 2.0221e+02, 2.2089e+02, 2.2048e+02],\n",
      "        [5.8575e+01, 1.0823e+00, 1.6366e+02, 2.9003e+01],\n",
      "        [1.5323e+00, 6.2139e-01, 4.9521e+01, 1.9608e+01],\n",
      "        [2.2877e+01, 1.3574e+02, 1.6611e+02, 2.2057e+02],\n",
      "        [9.8175e+01, 1.0449e+02, 1.9088e+02, 1.5520e+02],\n",
      "        [1.5722e+02, 5.5096e+01, 2.0208e+02, 7.4050e+01],\n",
      "        [4.2633e+01, 7.0986e+01, 1.4687e+02, 1.2445e+02],\n",
      "        [1.0410e+02, 2.2825e+01, 1.4852e+02, 4.0934e+01],\n",
      "        [1.1181e+02, 5.5144e+01, 1.5792e+02, 7.2200e+01],\n",
      "        [8.5850e+01, 5.9838e-01, 1.1987e+02, 3.0339e+01],\n",
      "        [1.7616e+02, 6.2446e+01, 2.2049e+02, 8.1831e+01],\n",
      "        [1.5026e+02, 1.2043e+02, 1.9130e+02, 1.3943e+02],\n",
      "        [4.0325e+01, 1.8609e+02, 1.3949e+02, 2.2400e+02],\n",
      "        [4.1412e+01, 1.9134e+01, 8.3737e+01, 3.6379e+01],\n",
      "        [9.7385e+01, 1.2098e+02, 1.9043e+02, 1.7294e+02],\n",
      "        [1.2758e+02, 1.0289e+00, 1.7220e+02, 2.1074e+01],\n",
      "        [0.0000e+00, 6.8852e+01, 1.2655e+02, 1.5845e+02],\n",
      "        [1.6795e+02, 1.5054e+02, 2.1102e+02, 1.6803e+02],\n",
      "        [2.7911e-01, 2.1339e+02, 5.2076e+01, 2.2400e+02],\n",
      "        [3.1239e+01, 3.2539e+01, 1.1646e+02, 6.9859e+01],\n",
      "        [2.0253e-02, 5.1409e+01, 1.5588e+01, 1.0758e+02],\n",
      "        [1.5409e+02, 1.3621e+02, 2.2400e+02, 2.2400e+02],\n",
      "        [1.2987e+02, 1.0041e+01, 2.0360e+02, 4.8160e+01],\n",
      "        [3.8150e+01, 6.1892e+01, 8.1980e+01, 8.0263e+01],\n",
      "        [2.0199e+01, 7.4140e-01, 5.4244e+01, 3.5050e+01],\n",
      "        [6.0113e+01, 1.4616e+01, 1.4707e+02, 6.1436e+01],\n",
      "        [1.6674e+02, 1.8376e+02, 2.1020e+02, 2.0373e+02],\n",
      "        [4.7135e+01, 7.7372e-01, 9.3348e+01, 1.8119e+01],\n",
      "        [3.5153e+01, 1.1989e+02, 8.0915e+01, 1.3894e+02],\n",
      "        [4.7805e+00, 1.9739e+02, 1.0124e+02, 2.2400e+02],\n",
      "        [8.2978e+01, 2.1137e+02, 1.3302e+02, 2.2400e+02],\n",
      "        [2.7051e-01, 5.0531e+01, 5.7895e+01, 1.6948e+02],\n",
      "        [3.5279e+01, 4.6527e+01, 1.2910e+02, 9.5807e+01],\n",
      "        [4.7576e+01, 1.8394e+02, 9.1114e+01, 2.0225e+02],\n",
      "        [1.0944e+01, 8.6797e+01, 1.0584e+02, 1.3550e+02],\n",
      "        [1.0645e+02, 1.4297e+02, 1.4624e+02, 1.6181e+02],\n",
      "        [0.0000e+00, 1.9756e+02, 4.8563e+01, 2.1656e+02],\n",
      "        [1.5176e+02, 1.8787e+02, 2.0082e+02, 2.0570e+02],\n",
      "        [1.7671e+02, 1.4619e+02, 2.2400e+02, 1.6760e+02],\n",
      "        [6.1373e+01, 9.8419e+01, 2.0434e+02, 1.8986e+02],\n",
      "        [2.1110e+01, 9.9343e+01, 6.4359e+01, 1.1814e+02],\n",
      "        [0.0000e+00, 7.0723e+00, 6.5062e+01, 5.8414e+01],\n",
      "        [3.5470e+01, 1.6845e+02, 8.1118e+01, 1.8548e+02],\n",
      "        [1.1982e+02, 1.3944e+02, 1.6006e+02, 1.5637e+02],\n",
      "        [6.8459e+01, 1.2103e+02, 1.9296e+02, 2.2400e+02],\n",
      "        [1.5826e+01, 1.6851e+02, 5.8791e+01, 1.8483e+02],\n",
      "        [1.2273e+02, 1.1215e+02, 1.6505e+02, 1.3138e+02],\n",
      "        [9.6208e+01, 1.6001e+02, 1.8934e+02, 2.0866e+02],\n",
      "        [6.1079e+01, 2.0475e+02, 1.0497e+02, 2.2342e+02],\n",
      "        [1.7473e+02, 2.8459e+01, 2.1808e+02, 4.6372e+01],\n",
      "        [1.6206e+02, 1.5967e+02, 2.2400e+02, 1.9315e+02],\n",
      "        [8.6179e+01, 9.4297e+01, 1.3154e+02, 1.1178e+02],\n",
      "        [8.8570e+01, 2.0292e+01, 2.2400e+02, 1.1352e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.2483e+01, 3.2367e+01],\n",
      "        [1.0224e+02, 2.1685e+01, 1.8114e+02, 6.2017e+01],\n",
      "        [2.8428e+01, 8.4951e+00, 7.5919e+01, 2.5191e+01],\n",
      "        [1.6791e+02, 2.0023e+02, 2.1121e+02, 2.1812e+02],\n",
      "        [1.1152e+02, 1.0432e+00, 1.4532e+02, 3.0884e+01],\n",
      "        [2.5692e+01, 1.7882e+02, 7.0828e+01, 1.9639e+02],\n",
      "        [1.7469e+02, 1.7165e+02, 2.1772e+02, 1.8964e+02],\n",
      "        [7.0110e+01, 9.7177e+01, 1.6782e+02, 1.4637e+02],\n",
      "        [0.0000e+00, 1.1372e+02, 9.4802e+01, 2.0193e+02],\n",
      "        [8.0017e+01, 7.0559e+01, 1.8058e+02, 1.2500e+02],\n",
      "        [0.0000e+00, 1.8681e+02, 2.7359e+01, 2.2400e+02],\n",
      "        [1.1321e+02, 7.2991e-01, 1.5819e+02, 1.8829e+01],\n",
      "        [6.6537e+01, 6.5689e+01, 1.1504e+02, 8.3995e+01],\n",
      "        [6.6374e+01, 1.3477e+02, 1.1555e+02, 1.5213e+02]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.4274, 0.4123, 0.4119, 0.4074, 0.3978, 0.3974, 0.3963, 0.3923, 0.3923,\n",
      "        0.3922, 0.3908, 0.3872, 0.3845, 0.3840, 0.3838, 0.3833, 0.3827, 0.3819,\n",
      "        0.3812, 0.3787, 0.3786, 0.3785, 0.3781, 0.3772, 0.3772, 0.3748, 0.3742,\n",
      "        0.3739, 0.3724, 0.3721, 0.3712, 0.3708, 0.3705, 0.3678, 0.3674, 0.3670,\n",
      "        0.3667, 0.3666, 0.3648, 0.3647, 0.3647, 0.3637, 0.3636, 0.3634, 0.3631,\n",
      "        0.3629, 0.3629, 0.3626, 0.3625, 0.3624, 0.3624, 0.3614, 0.3614, 0.3608,\n",
      "        0.3604, 0.3600, 0.3599, 0.3598, 0.3593, 0.3591, 0.3590, 0.3584, 0.3583,\n",
      "        0.3581, 0.3580, 0.3580, 0.3578, 0.3577, 0.3577, 0.3576, 0.3576, 0.3570,\n",
      "        0.3564, 0.3564, 0.3560, 0.3559, 0.3556, 0.3546, 0.3546, 0.3544, 0.3543,\n",
      "        0.3541, 0.3540, 0.3537, 0.3534, 0.3524, 0.3516, 0.3513, 0.3512, 0.3512,\n",
      "        0.3511, 0.3510, 0.3510, 0.3510, 0.3507, 0.3501, 0.3498, 0.3496, 0.3492,\n",
      "        0.3490], device='cuda:0'), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = build_swin_maskrcnn(num_classes=2).to(device)\n",
    "    x = [torch.randn(3, 224, 224, device=device)]\n",
    "    targets = [{\n",
    "        \"boxes\": torch.tensor([[50,50,150,150]], dtype=torch.float32, device=device),\n",
    "        \"labels\": torch.tensor([1], device=device),\n",
    "        \"masks\": torch.randint(0,2,(1,224,224), device=device, dtype=torch.uint8),\n",
    "    }]\n",
    "\n",
    "    model.train()\n",
    "    losses = model(x, targets)  # forward pass -> dict of losses\n",
    "    print(losses)  # e.g. { 'loss_classifier':..., 'loss_box_reg':..., ... }\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(x)  # inference\n",
    "        print(preds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
