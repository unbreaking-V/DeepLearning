{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange\n",
    "from torchvision.ops.stochastic_depth import StochasticDepth # Add stochastic depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Partition + Linear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEmbedding(nn.Module):\n",
    "\n",
    "  \"\"\"\n",
    "  input shape -> (b,c,h,w)\n",
    "  output shape -> (b, (h/4 * w/4), C)\n",
    "\n",
    "  Where:\n",
    "\n",
    "  b - batch size\n",
    "  c - number of channels\n",
    "  h - height of the image\n",
    "  w - width of the image\n",
    "  C - number of channels in the output\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, patch_size = 4, C = 96):\n",
    "      super().__init__()\n",
    "      self.linear_embedding = nn.Conv2d(3,C, kernel_size=patch_size, stride=patch_size)\n",
    "      self.layer_norm = nn.LayerNorm(C)\n",
    "      self.relu = nn.ReLU() # Tej funkcji aktywacji nie ma w oryginalnym modelu, ale jest ona potrzebna do poprawnego działania modelu\n",
    "\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.linear_embedding(x)\n",
    "    x = rearrange(x, 'b c h w -> b (h w) c')  # spłaszczenie wymiarów przestrzennych obrazu przy pomocy mnożenia h i w\n",
    "    x = self.layer_norm(x) # normalizacja\n",
    "    x = self.relu(x) # funkcja aktywacji (dodanie nieliniowości) (nie ma jej w oryginalnym modelu)\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Merging Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "  \"\"\"\n",
    "  Reduces tokens by a factor of 4 (2x2 patches) and doubles embedding dimension.\n",
    "\n",
    "\n",
    "  input shape -> (b (h w) c)\n",
    "  output shape -> (b (h/2 * w/2) C*2)\n",
    "\n",
    "  Where:\n",
    "\n",
    "  b - batch size\n",
    "  c - number of channels\n",
    "  h - height of the image\n",
    "  w - width of the image\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, C) -> None:\n",
    "     super().__init__()\n",
    "     self.linear_layer = nn.Linear(C*4, C*2) # podwajamy wymiar embeddingów\n",
    "     self.layer_norm = nn.LayerNorm(2 * C) # normalizacja\n",
    "\n",
    "  def forward(self, x):\n",
    "    height = width = int(math.sqrt(x.shape[1])/ 2) # obliczamy nową wysokość i szerokość obrazu\n",
    "    x = rearrange(x, 'b (h s1 w s2) c -> b (h w) (s2 s1 c)', s1=2, s2=2, h=height, w=width)\n",
    "    x = self.linear_layer(x)\n",
    "    x = self.layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifted Window Attention Mechanism (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedWindowMSA(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input shape -> (b , (h*w), C)\n",
    "    output shape -> (b , (h*w), C)\n",
    "\n",
    "    Where:\n",
    "\n",
    "    b - batch size\n",
    "    h - height of the image\n",
    "    w - width of the image\n",
    "    C - number of channels in the output\n",
    "    \"\"\"\n",
    "      \n",
    "    def __init__(self, embed_dim, num_heads, window_size=7, mask=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim # wymiar embeddingów\n",
    "        self.num_heads = num_heads # liczba głów\n",
    "        self.window_size = window_size # rozmiar okna\n",
    "        self.mask = mask # maska (True/False)\n",
    "        self.proj1 = nn.Linear(embed_dim, 3*embed_dim) # projekcja wejścia\n",
    "        self.proj2 = nn.Linear(embed_dim, embed_dim) # projekcja wyjścia\n",
    "        self.embeddings = RelativeEmbeddings() \n",
    "\n",
    "    def forward(self, x):\n",
    "        h_dim = self.embed_dim / self.num_heads # obliczamy wymiar pojedynczej głowy\n",
    "        height = width = int(math.sqrt(x.shape[1])) \n",
    "        x = self.proj1(x) \n",
    "        x = rearrange(x, 'b (h w) (c K) -> b h w c K', K=3, h=height, w=width) # zmiana wymiarów, gdzie K to liczba macierzy Q,K,V\n",
    " \n",
    "        if self.mask: # jeśli maska jest True, to wykonujemy przesunięcie okna o połowę\n",
    "            x = torch.roll(x, (-self.window_size//2, -self.window_size//2), dims=(1,2))\n",
    "\n",
    "        # zmiana wymiarów\n",
    "        x = rearrange(x, 'b (h m1) (w m2) (H E) K -> b H h w (m1 m2) E K', H=self.num_heads, m1=self.window_size, m2=self.window_size)\n",
    "       \n",
    "        # podział na macierze Q,K,V\n",
    "        Q, K, V = x.chunk(3, dim=6)\n",
    "        Q, K, V = Q.squeeze(-1), K.squeeze(-1), V.squeeze(-1)\n",
    "        attention_scores = (Q @ K.transpose(4,5)) / math.sqrt(h_dim) # obliczamy self-attention score\n",
    "        attention_scores = self.embeddings(attention_scores) # dodajemy embeddingsy\n",
    "\n",
    "        '''\n",
    "        H - attention heads \n",
    "        h,w - vertical and horizontal dimensions of the image\n",
    "        (m1 m2) - total size of the window\n",
    "        E - head dimension\n",
    "        K = 3 - constant to break our matrix into 3 Q,K,V matricies\n",
    "      \n",
    "        shape of attention_scores = (b, H, h, w, (m1*m2), (m1*m2))\n",
    "        we simply have to generate our row/column masks and apply them\n",
    "        to the last row and columns of windows which are [:,:,-1,:] and [:,:,:,-1]\n",
    "        \n",
    "        '''\n",
    "\n",
    "        if self.mask: # jeśli maska jest True, to wykonujemy maskowanie ostatnich wierszy i kolumn w oknie \n",
    "            row_mask = torch.zeros((self.window_size**2, self.window_size**2)).cuda()\n",
    "            row_mask[-self.window_size * (self.window_size//2):, 0:-self.window_size * (self.window_size//2)] = float('-inf')\n",
    "            row_mask[0:-self.window_size * (self.window_size//2), -self.window_size * (self.window_size//2):] = float('-inf')\n",
    "            column_mask = rearrange(row_mask, '(r w1) (c w2) -> (w1 r) (w2 c)', w1=self.window_size, w2=self.window_size).cuda()\n",
    "            attention_scores[:, :, -1, :] += row_mask\n",
    "            attention_scores[:, :, :, -1] += column_mask\n",
    "\n",
    "        attention = F.softmax(attention_scores, dim=-1) @ V # Softmax i mnożenie przez V \n",
    "        x = rearrange(attention, 'b H h w (m1 m2) E -> b (h m1) (w m2) (H E)', m1=self.window_size, m2=self.window_size)\n",
    "\n",
    "        if self.mask: # Z powrotem przesuwamy okno o połowę\n",
    "            x = torch.roll(x, (self.window_size//2, self.window_size//2), (1,2))\n",
    "\n",
    "        x = rearrange(x, 'b h w c -> b (h w) c')\n",
    "        return self.proj2(x) # projekcja wyjścia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Position Embeddings (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeEmbeddings(nn.Module):\n",
    "    def __init__(self, window_size=7):\n",
    "        super().__init__()\n",
    "        B = nn.Parameter(torch.randn(2*window_size-1, 2*window_size-1))\n",
    "        x = torch.arange(1,window_size+1,1/window_size)\n",
    "        x = (x[None, :]-x[:, None]).int()\n",
    "        y = torch.concat([torch.arange(1,window_size+1)] * window_size)\n",
    "        y = (y[None, :]-y[:, None])\n",
    "        self.embeddings = nn.Parameter((B[x[:,:], y[:,:]]), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer Block v2 \n",
    "\n",
    "The main difference of the Swin Transformer block of the second version is the change of the normalization order. The normalization layer was moved before the skip connection adder, which reduced the amplitude of activations and provided more stable and efficient learning.\n",
    "\n",
    "In addition, a stochastic drop path operation was added to the block to improve regularization. This is especially important for deep models and transformers, where this approach has been shown to perform better according to research.\n",
    "\n",
    "Initialization of weights and biases was also introduced, which promotes stable learning and accelerates model convergence due to correct distribution of initial parameters\n",
    "\n",
    "![image.png](/home/wladyka/Swin-Transformer/images/swin_transformer_block_v2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size, mask):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.stochastic_depth = StochasticDepth(0.2, \"row\") # Stochastic Depth with 0.2 probability of dropping out a row for tiny version of Swin Transformer\n",
    "\n",
    "        self.WMSA = ShiftedWindowMSA(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=mask)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.1), # Default dropout probability is 0.0 in the torchvision implementation\n",
    "            nn.Linear(embed_dim*4, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Initialization of weights and biases (bias) in linear layers \n",
    "        for m in self.MLP:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight) # Xavier initialization for weights, which prevents the disappearance or explosion of gradients during training.\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6) # Set a small offset, to have a small impact in the initial stages of training.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Attention path with pre-normalization \n",
    "        res1 = x # Save input for the skip connection\n",
    "        x = self.stochastic_depth(self.layer_norm(self.WMSA(x))) # Attention block with LayerNorm and Stochastic Depth(more efficient than Dropout for training transformers)\n",
    "        x = res1 + x # Skip connection\n",
    "\n",
    "        # MLP path with pre-normalization\n",
    "        res2 = x  # Save intermediate result for skip connection\n",
    "        x = self.stochastic_depth(self.layer_norm(self.MLP(x))) # MLP block with LayerNorm and Dropout\n",
    "        x = res2 + x  # Skip connection\n",
    "\n",
    "        return x\n",
    "    \n",
    "class AlternatingEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size=7):\n",
    "        super().__init__()\n",
    "        self.WSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=False)\n",
    "        self.SWSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.SWSA(self.WSA(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Swin-Transformer Class (Review) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerTiny(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Embedding = SwinEmbedding() # Embedding layer\n",
    "        self.PatchMerge1 = PatchMerging(96)\n",
    "        self.PatchMerge2 = PatchMerging(192)\n",
    "        self.PatchMerge3 = PatchMerging(384)\n",
    "        self.Stage1 = AlternatingEncoderBlock(96, 3)\n",
    "        self.Stage2 = AlternatingEncoderBlock(192, 6)\n",
    "        self.Stage3_1 = AlternatingEncoderBlock(384, 12)\n",
    "        self.Stage3_2 = AlternatingEncoderBlock(384, 12)\n",
    "        self.Stage3_3 = AlternatingEncoderBlock(384, 12)\n",
    "        self.Stage4 = AlternatingEncoderBlock(768, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x)\n",
    "        x = self.PatchMerge1(self.Stage1(x))\n",
    "        x = self.PatchMerge2(self.Stage2(x))\n",
    "        x = self.Stage3_1(x)\n",
    "        x = self.Stage3_2(x)\n",
    "        x = self.Stage3_3(x)\n",
    "        x = self.PatchMerge3(x)\n",
    "        x = self.Stage4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 768])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    x = torch.randn((1,3,224,224)).cuda()\n",
    "    model = SwinTransformerTiny().cuda()\n",
    "    print(model(x).shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
