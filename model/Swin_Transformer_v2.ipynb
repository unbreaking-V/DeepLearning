{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange\n",
    "from torchvision.ops.stochastic_depth import StochasticDepth # Add stochastic depth\n",
    "from typing import List "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Partition + Linear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEmbedding(nn.Module):\n",
    "\n",
    "  \"\"\"\n",
    "  input shape -> (b,c,h,w)\n",
    "  output shape -> (b, (h/4 * w/4), C)\n",
    "\n",
    "  Where:\n",
    "\n",
    "  b - batch size\n",
    "  c - number of channels\n",
    "  h - height of the image\n",
    "  w - width of the image\n",
    "  C - number of channels in the output\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, patch_size = 4, C = 96):\n",
    "      super().__init__()\n",
    "      self.linear_embedding = nn.Conv2d(3,C, kernel_size=patch_size, stride=patch_size)\n",
    "      self.layer_norm = nn.LayerNorm(C)\n",
    "      self.relu = nn.ReLU() # Tej funkcji aktywacji nie ma w oryginalnym modelu, ale jest ona potrzebna do poprawnego działania modelu\n",
    "\n",
    "  \n",
    "  def forward(self,x):\n",
    "    x = self.linear_embedding(x)\n",
    "    x = rearrange(x, 'b c h w -> b (h w) c')  # spłaszczenie wymiarów przestrzennych obrazu przy pomocy mnożenia h i w\n",
    "    x = self.layer_norm(x) # normalizacja\n",
    "    x = self.relu(x) # funkcja aktywacji (dodanie nieliniowości) (nie ma jej w oryginalnym modelu)\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Merging Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "  \"\"\"\n",
    "  Reduces tokens by a factor of 4 (2x2 patches) and doubles embedding dimension.\n",
    "\n",
    "\n",
    "  input shape -> (b (h w) c)\n",
    "  output shape -> (b (h/2 * w/2) C*2)\n",
    "\n",
    "  Where:\n",
    "\n",
    "  b - batch size\n",
    "  c - number of channels\n",
    "  h - height of the image\n",
    "  w - width of the image\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, C) -> None:\n",
    "     super().__init__()\n",
    "     self.linear_layer = nn.Linear(C*4, C*2) # podwajamy wymiar embeddingów\n",
    "     self.layer_norm = nn.LayerNorm(2 * C) # normalizacja\n",
    "\n",
    "  def forward(self, x):\n",
    "    height = width = int(math.sqrt(x.shape[1])/ 2) # obliczamy nową wysokość i szerokość obrazu\n",
    "    x = rearrange(x, 'b (h s1 w s2) c -> b (h w) (s2 s1 c)', s1=2, s2=2, h=height, w=width)\n",
    "    x = self.linear_layer(x)\n",
    "    x = self.layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifted Window Attention Mechanism (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedWindowMSA(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    input shape -> (b , (h*w), C)\n",
    "    output shape -> (b , (h*w), C)\n",
    "\n",
    "    Where:\n",
    "\n",
    "    b - batch size\n",
    "    h - height of the image\n",
    "    w - width of the image\n",
    "    C - number of channels in the output\n",
    "    \"\"\"\n",
    "      \n",
    "    def __init__(self, embed_dim, num_heads, window_size=7, mask=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim # wymiar embeddingów\n",
    "        self.num_heads = num_heads # liczba głów\n",
    "        self.window_size = window_size # rozmiar okna\n",
    "        self.mask = mask # maska (True/False)\n",
    "        self.qkv = nn.Linear(embed_dim, 3*embed_dim) # projekcja wejścia\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim) # projekcja wyjścia\n",
    "        self.embeddings = RelativeEmbeddings()\n",
    "        self.embeddings_v2 = RelativeEmbeddingsv2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_dim = self.embed_dim / self.num_heads # obliczamy wymiar pojedynczej głowy\n",
    "        height = width = int(math.sqrt(x.shape[1])) \n",
    "        x = self.qkv(x) \n",
    "        x = rearrange(x, 'b (h w) (c K) -> b h w c K', K=3, h=height, w=width) # zmiana wymiarów, gdzie K to liczba macierzy Q,K,V\n",
    " \n",
    "        if self.mask: # jeśli maska jest True, to wykonujemy przesunięcie okna o połowę\n",
    "            x = torch.roll(x, (-self.window_size//2, -self.window_size//2), dims=(1,2))\n",
    "\n",
    "        # zmiana wymiarów\n",
    "        x = rearrange(x, 'b (h m1) (w m2) (H E) K -> b H h w (m1 m2) E K', H=self.num_heads, m1=self.window_size, m2=self.window_size)\n",
    "       \n",
    "        # podział na macierze Q,K,V\n",
    "        Q, K, V = x.chunk(3, dim=6)\n",
    "        Q, K, V = Q.squeeze(-1), K.squeeze(-1), V.squeeze(-1)\n",
    "        attention_scores = (Q @ K.transpose(4,5)) / math.sqrt(h_dim) # obliczamy self-attention score\n",
    "        # print(attention_scores.shape)\n",
    "        # print(\"add embeddings\")\n",
    "        # print( self.embeddings(attention_scores).shape)\n",
    "        attention_scores = self.embeddings(attention_scores) # dodajemy embeddingsy\n",
    "\n",
    "        #print(self.embeddings_v2(attention_scores).shape)\n",
    "\n",
    "\n",
    "        '''\n",
    "        H - attention heads \n",
    "        h,w - vertical and horizontal dimensions of the image\n",
    "        (m1 m2) - total size of the window\n",
    "        E - head dimension\n",
    "        K = 3 - constant to break our matrix into 3 Q,K,V matricies\n",
    "      \n",
    "        shape of attention_scores = (b, H, h, w, (m1*m2), (m1*m2))\n",
    "        we simply have to generate our row/column masks and apply them\n",
    "        to the last row and columns of windows which are [:,:,-1,:] and [:,:,:,-1]\n",
    "        \n",
    "        '''\n",
    "\n",
    "        if self.mask: # jeśli maska jest True, to wykonujemy maskowanie ostatnich wierszy i kolumn w oknie \n",
    "            row_mask = torch.zeros((self.window_size**2, self.window_size**2)).cuda()\n",
    "            row_mask[-self.window_size * (self.window_size//2):, 0:-self.window_size * (self.window_size//2)] = float('-inf')\n",
    "            row_mask[0:-self.window_size * (self.window_size//2), -self.window_size * (self.window_size//2):] = float('-inf')\n",
    "            column_mask = rearrange(row_mask, '(r w1) (c w2) -> (w1 r) (w2 c)', w1=self.window_size, w2=self.window_size).cuda()\n",
    "            attention_scores[:, :, -1, :] += row_mask\n",
    "            attention_scores[:, :, :, -1] += column_mask\n",
    "\n",
    "        attention = F.softmax(attention_scores, dim=-1) @ V # Softmax i mnożenie przez V \n",
    "        x = rearrange(attention, 'b H h w (m1 m2) E -> b (h m1) (w m2) (H E)', m1=self.window_size, m2=self.window_size)\n",
    "\n",
    "        if self.mask: # Z powrotem przesuwamy okno o połowę\n",
    "            x = torch.roll(x, (self.window_size//2, self.window_size//2), (1,2))\n",
    "\n",
    "        x = rearrange(x, 'b h w c -> b (h w) c')\n",
    "        return self.proj(x) # projekcja wyjścia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative Position Embeddings (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeEmbeddings(nn.Module):\n",
    "    def __init__(self, window_size=7):\n",
    "        super().__init__()\n",
    "        B = nn.Parameter(torch.randn(2*window_size-1, 2*window_size-1))\n",
    "        x = torch.arange(1,window_size+1,1/window_size)\n",
    "        x = (x[None, :]-x[:, None]).int()\n",
    "        y = torch.concat([torch.arange(1,window_size+1)] * window_size)\n",
    "        y = (y[None, :]-y[:, None])\n",
    "        self.embeddings = nn.Parameter((B[x[:,:], y[:,:]]), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"RelativeEmbeddings shape: {self.embeddings.shape} + x shape: {x.shape} = {(self.embeddings + x).shape}\")\n",
    "        return  self.embeddings  + x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeEmbeddingsv2(nn.Module):\n",
    "    \"\"\"\n",
    "    See :func:`shifted_window_attention`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim = 96,\n",
    "        window_size = [7,7],\n",
    "        qkv_bias: bool = True,\n",
    "        attention_dropout: float = 0.0,\n",
    "        dropout: float = 0.0,\n",
    "        num_heads: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.define_relative_position_bias_table()\n",
    "        self.define_relative_position_index()\n",
    "\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n",
    "        \n",
    "        # mlp to generate continuous relative position bias\n",
    "        self.cpb_mlp = nn.Sequential(\n",
    "                nn.Linear(2, 512, bias=True), nn.ReLU(inplace=True), nn.Linear(512, num_heads, bias=False)\n",
    "            )\n",
    "        if qkv_bias:\n",
    "            length = self.qkv.bias.numel() // 3\n",
    "            self.qkv.bias[length : 2 * length].data.zero_()\n",
    "\n",
    "    def _get_relative_position_bias(\n",
    "        relative_position_bias_table: torch.Tensor, relative_position_index: torch.Tensor, window_size: List[int]\n",
    "    ) -> torch.Tensor:\n",
    "        N = window_size[0] * window_size[1]\n",
    "        relative_position_bias = relative_position_bias_table[relative_position_index]  # type: ignore[index]\n",
    "        relative_position_bias = relative_position_bias.view(N, N, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().unsqueeze(0)\n",
    "        return relative_position_bias\n",
    "\n",
    "    def define_relative_position_bias_table(self):\n",
    "        # get relative_coords_table\n",
    "        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n",
    "        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n",
    "        relative_coords_table = torch.stack(torch.meshgrid([relative_coords_h, relative_coords_w], indexing=\"ij\"))\n",
    "        relative_coords_table = relative_coords_table.permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n",
    "\n",
    "        relative_coords_table[:, :, :, 0] /= self.window_size[0] - 1\n",
    "        relative_coords_table[:, :, :, 1] /= self.window_size[1] - 1\n",
    "\n",
    "        relative_coords_table *= 8  # normalize to -8, 8\n",
    "        relative_coords_table = (\n",
    "            torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / 3.0\n",
    "        )\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n",
    "\n",
    "    def define_relative_position_index(self):\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1).flatten()  # Wh*Ww*Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "    def get_relative_position_bias(self) -> torch.Tensor:\n",
    "        relative_position_bias = _get_relative_position_bias(\n",
    "            self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads),\n",
    "            self.relative_position_index,  # type: ignore[arg-type]\n",
    "            self.window_size,\n",
    "        )\n",
    "        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n",
    "        return relative_position_bias\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "        \"\"\"\n",
    "        relative_position_bias = self.get_relative_position_bias()\n",
    "        print(f\"RelativeEmbeddingsV2 shape: {relative_position_bias.shape} + x shape: {x.shape} = {(relative_position_bias + x).shape}\")\n",
    "        return relative_position_bias + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer Block v2 \n",
    "\n",
    "The main difference of the Swin Transformer block of the second version is the change of the normalization order. The normalization layer was moved before the skip connection adder, which reduced the amplitude of activations and provided more stable and efficient learning.\n",
    "\n",
    "In addition, a stochastic drop path operation was added to the block to improve regularization. This is especially important for deep models and transformers, where this approach has been shown to perform better according to research.\n",
    "\n",
    "Initialization of weights and biases was also introduced, which promotes stable learning and accelerates model convergence due to correct distribution of initial parameters\n",
    "\n",
    "![image.png](/home/wladyka/Swin-Transformer/images/swin_transformer_block_v2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size, mask, sd_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.stochastic_depth = StochasticDepth(sd_prob, \"row\") # Stochastic Depth with 0.1 probability of dropping out a row for tiny version of Swin Transformer\n",
    "\n",
    "        self.WMSA = ShiftedWindowMSA(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=mask)\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.1), # Default dropout probability is 0.0 in the torchvision implementation\n",
    "            nn.Linear(embed_dim*4, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Initialization of weights and biases (bias) in linear layers \n",
    "        for m in self.MLP:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight) # Xavier initialization for weights, which prevents the disappearance or explosion of gradients during training.\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6) # Set a small offset, to have a small impact in the initial stages of training.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Attention path with pre-normalization \n",
    "        res1 = x # Save input for the skip connection\n",
    "        x = self.stochastic_depth(self.layer_norm(self.WMSA(x))) # Attention block with LayerNorm and Stochastic Depth(more efficient than Dropout for training transformers)\n",
    "        x = res1 + x # Residual connection\n",
    "\n",
    "        # MLP path with pre-normalization\n",
    "        res2 = x  # Save intermediate result for skip connection\n",
    "        x = self.stochastic_depth(self.layer_norm(self.MLP(x))) # MLP block with LayerNorm and Dropout\n",
    "        x = res2 + x  # Residual connection\n",
    "\n",
    "        return x\n",
    "    \n",
    "class AlternatingEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, sd_prob, window_size=7):\n",
    "        super().__init__()\n",
    "        self.WSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=False, sd_prob=sd_prob[0])\n",
    "        self.SWSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=True, sd_prob=sd_prob[1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.SWSA(self.WSA(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Swin-Transformer Class v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerTiny(nn.Module):\n",
    "    def __init__(self, depth=[2, 2, 6, 2], embed_dim=96, stochastic_depth_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.Embedding = SwinEmbedding()  # Embedding layer\n",
    "\n",
    "        # Calculate total number of blocks\n",
    "        total_stage_blocks = sum(depth)\n",
    "        stage_block_id = 0\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "\n",
    "        in_channels = embed_dim\n",
    "        for i_stage, num_blocks in enumerate(depth):\n",
    "            temp_sd_prob = []\n",
    "            for _ in range(num_blocks):\n",
    "                # Calculate probability for the current layer\n",
    "                sd_prob = stochastic_depth_prob * float(stage_block_id) / (total_stage_blocks - 1)\n",
    "                temp_sd_prob.append(sd_prob)\n",
    "                stage_block_id += 1\n",
    "\n",
    "            #Add alternating encoder blocks recording to the depth list divided by 2, because each block has 2 sub-blocks\n",
    "            sd_prob = [temp_sd_prob[i:i+2] for i in range(0, len(temp_sd_prob), 2)]\n",
    "            for _ in range(int(num_blocks / 2)):\n",
    "                num_heads = in_channels // 32\n",
    "                #print(f\"AlternatingEncoderBlock({in_channels}, {num_heads}, {sd_prob[0]})\") # Debug\n",
    "                self.stages.append(\n",
    "                    AlternatingEncoderBlock(in_channels, num_heads, sd_prob[0])\n",
    "                )\n",
    "                sd_prob.pop(0)\n",
    "                    \n",
    "            # Add patch merging layer if this is not the last stage\n",
    "            if i_stage < len(depth) - 1:\n",
    "                self.stages.append(PatchMerging(in_channels))\n",
    "                #print(f\"PatchMerging({in_channels})\") # Debug\n",
    "                in_channels *= 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x) \n",
    "        for stage in self.stages: \n",
    "            x = stage(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 3, 8, 8, 49, 49]) = torch.Size([1, 3, 8, 8, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 3, 8, 8, 49, 49]) = torch.Size([1, 3, 8, 8, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 6, 4, 4, 49, 49]) = torch.Size([1, 6, 4, 4, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 6, 4, 4, 49, 49]) = torch.Size([1, 6, 4, 4, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 12, 2, 2, 49, 49]) = torch.Size([1, 12, 2, 2, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 12, 2, 2, 49, 49]) = torch.Size([1, 12, 2, 2, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 12, 2, 2, 49, 49]) = torch.Size([1, 12, 2, 2, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 12, 2, 2, 49, 49]) = torch.Size([1, 12, 2, 2, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 12, 2, 2, 49, 49]) = torch.Size([1, 12, 2, 2, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 12, 2, 2, 49, 49]) = torch.Size([1, 12, 2, 2, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 24, 1, 1, 49, 49]) = torch.Size([1, 24, 1, 1, 49, 49])\n",
      "RelativeEmbeddings shape: torch.Size([49, 49]) + x shape: torch.Size([1, 24, 1, 1, 49, 49]) = torch.Size([1, 24, 1, 1, 49, 49])\n",
      "torch.Size([1, 49, 768])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    x = torch.randn((1,3,224,224)).cuda()\n",
    "    model = SwinTransformerTiny().cuda()\n",
    "    print(model(x).shape)\n",
    "    # model = RelativeEmbeddingsv2().cuda()\n",
    "    # print(model(x).shape)\n",
    "    # # model = RelativeEmbeddings().cuda()\n",
    "    # # print(model(x).shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
