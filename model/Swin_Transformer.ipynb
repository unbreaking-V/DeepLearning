{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDrdNNEUI5aE"
      },
      "source": [
        "# Imports\n",
        "\n",
        "* Importy PyTorch: torch, torch.nn i torch.nn.functional używane do podstawowych operacji tensorowych i modułów sieci neuronowych.\n",
        "* Import math służy do normalizacji pierwiastka kwadratowego w attention.\n",
        "* Einops rearrange służy do przekształcania i permutacji tensorów w przyjazny dla czytelnika i wydajny sposób.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5EUd4fk8Izsy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.ops.stochastic_depth import StochasticDepth \n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "from torchvision.ops import FeaturePyramidNetwork\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUEtrxD52Qv8"
      },
      "source": [
        "# Patch Partition + Linear Embedding\n",
        "\n",
        "---\n",
        "„Najpierw dzieli wejściowy obraz RGB na nienakładające się patchs za pomocą modułu dzielenia patch, takiego jak ViT. Każda patch jest traktowana jako „token”, a jej cecha jest ustawiana jako konkatenacja surowych wartości RGB pikseli. W naszej implementacji używamy patch o rozmiarze 4 × 4, a zatem wymiar funkcji każdego patcha wynosi 4 × 4 × 3 = 48. Liniowa warstwa osadzania jest stosowana na tej surowej funkcji, aby rzutować ją na dowolny wymiar (oznaczony jako C)”.\n",
        "\n",
        "---\n",
        "\n",
        "Gdzie C jest hyperparametrem, który określa wymiar osadzenia. W naszym przypadku C = 96, dla modelu Swin-Transformer(tiny).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![image](../images/Patch_Partition_Linear_Embedding.png)\n",
        "\n",
        "\n",
        "Podział patchy w stylu ViT i liniowe embeding można zrealizować za pomocą splotu z rozmiarem jądra, krokiem (stride) równym rozmiarowi patcha oraz wyjściowymi kanałami równymi \\(C\\). Wynikowy tensor ma wymiary \\(H/p * W/p * C\\), gdzie każdy „token” odpowiada liniowemu przekształceniu pikseli patcha. Wymiar embedings \\(C\\) to liczba cech (kanałów), które opisują każdą jednostkę w reprezentacji danych. W naszym przypadku \\(C = 96\\).\n",
        "\n",
        "Klasa **SwinEmbedding**, dziedzicząca z **nn.Module**, inicjalizuje:\n",
        "1. Warstwę splotu \\(p * p\\) (stride \\(p\\)), z kanałami wyjściowymi \\(C\\),\n",
        "2. **LayerNorm** dla wymiaru embeding \\(C\\),\n",
        "3. Funkcję aktywacji ReLU.\n",
        "\n",
        "W metodzie `forward` wejście jest przepuszczane przez splot, przekształcane i permutowane, łącząc \\(H, W\\) w \\(H * W / p^2\\), a wymiar osadzania \\(C\\) przesuwany na końcową pozycję. Na końcu stosowane są normalizacja i ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pWnJy2ODJ-Ak"
      },
      "outputs": [],
      "source": [
        "class SwinEmbedding(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  input shape -> (b,c,h,w)\n",
        "  output shape -> (b, h/4 , w/4, C)\n",
        "\n",
        "  Where:\n",
        "\n",
        "  b - batch size\n",
        "  h - height of the image\n",
        "  w - width of the image\n",
        "  C - number of channels\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, patch_size = 4, C = 96):\n",
        "      super().__init__()\n",
        "      self.linear_embedding = nn.Conv2d(3,C, kernel_size=patch_size, stride=patch_size)\n",
        "      self.layer_norm = nn.LayerNorm(C)\n",
        "      self.relu = nn.ReLU() # activation function (not present in the torchvision model)\n",
        "\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.linear_embedding(x) # image partitioning into patches\n",
        "    x = rearrange(x, 'b c h w -> b h w c')  # change the shape of the tensor\n",
        "    x = self.layer_norm(x) \n",
        "    x = self.relu(x) # activation function (not present in the torchvision model)\n",
        "\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPtP1bboQqEc"
      },
      "source": [
        "# Patch Merging Layer\n",
        "\n",
        "![image](../images/hearachical_system.png)\n",
        "\n",
        "Aby stworzyć hierarchiczną reprezentację, liczba tokenów jest zmniejszana przez warstwy scalania patchy, gdy sieć staje się głębsza. Pierwsza warstwa scalania patch'y łączy cechy każdej grupy 2 × 2 sąsiednich patch'y i stosuje warstwę liniową na 4C-wymiarowych połączonych cechach. Zmniejsza to liczbę tokenów o wielokrotność 2×2 = 4 (2-krotne zmniejszenie rozdzielczości), a wymiar wyjściowy jest ustawiony na 2C.\n",
        "Inicjalizujemy warstwę liniową z kanałami wejściowymi 4C do kanałów wyjściowych 2C i inicjalizujemy normę warstwy z wyjściowym rozmiarem osadzania. W naszej funkcji forward używamy einops rearrange do zmiany kształtu naszych tokenów z 2x2xC na 1x1x4C. Kończymy, przepuszczając nasze dane wejściowe przez projekcję liniową i normę warstwy.\n",
        "\n",
        "![image](../images/Patch_mergering.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "syDDKgD8MTW0"
      },
      "outputs": [],
      "source": [
        "class PatchMerging(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  Reduces tokens by a factor of 4 (2x2 patches) and doubles embedding dimension.\n",
        "\n",
        "\n",
        "  input shape -> (b h w c)\n",
        "  output shape -> (b h/2 w/2 C*2)\n",
        "\n",
        "  Where:\n",
        "\n",
        "  b - batch size\n",
        "  c - number of channels\n",
        "  h - height of the image\n",
        "  w - width of the image\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, C) -> None:\n",
        "     super().__init__()\n",
        "     self.linear_layer = nn.Linear(C*4, C*2) # Doubles the embedding dimension\n",
        "     self.layer_norm = nn.LayerNorm(2 * C) # Layer normalization\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = rearrange(x, 'b (h ph) (w pw) c -> b h w (ph pw c)', ph=2, pw=2) # Merge patches and double the embedding dimension\n",
        "    x = self.linear_layer(x) \n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfEJmwXVuHEe"
      },
      "source": [
        "# Shifted Window Attention Mechanism\n",
        "\n",
        "\n",
        "Zaczynamy od zainicjowania naszych parametrów embed_dim, num_heads i window_size oraz zdefiniowania dwóch projekcji liniowych. Pierwsza z nich to nasza projekcja z danych wejściowych do zapytań, kluczy i wartości, którą wykonujemy w jednej równoległej projekcji, więc rozmiar wyjściowy jest ustawiony na 3*C. Druga projekcja to projekcja liniowa zastosowana po obliczeniach uwagi. Projekcja ta służy do komunikacji między połączonymi równoległymi wielogłowicowymi jednostkami uwagi.\n",
        "\n",
        "Rozpoczynamy naszą funkcję do przodu, uzyskując rozmiar naszej głowy, wysokość i szerokość naszego wejścia, ponieważ potrzebujemy tych parametrów do zmiany układu. Następnie wykonujemy projekcję Q,K,V na naszym wejściu o kształcie ((h*w), c) do ((h*w), 3C). Nasz następny krok składa się z dwóch części, w których zmienimy nasze dane wejściowe ((h*w), C*3) na okna i równoległe głowice uwagi do naszych obliczeń uwagi.\n",
        "\n",
        "Pózniej rozbijamy naszą macierz na 3 macierze Q,K,V i obliczamy uwagę za pomocą standardowego wzoru uwagi:\n",
        "\n",
        "Formuła self-attention w mechanizmie transformera wygląda następująco:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Obliczanie Attention Scores**\n",
        "\n",
        "```python\n",
        "attention_scores = (Q @ K.transpose(4, 5)) / math.sqrt(h_dim)\n",
        "```\n",
        "\n",
        "Dla każdego tokena obliczamy podobieństwo (iloczyn skalarny) między wektorem zapytania  $Q$ a wszystkimi kluczami $K$. Następnie dzielimy przez $(\\sqrt{d_k})$, aby zachować stabilność gradientów. Pózniej wyniki zmarnalizowane poprzez  dzielienia na $d_k$, gdzie $d_k$ to wymiar wektorów $Q$ i $K$. Dzielimy przez $(\\sqrt{d_k})$, aby zachować stabilność gradientów.\n",
        "\n",
        "\n",
        "**Softmax i kontekst uwagi**\n",
        "\n",
        "```python\n",
        "attention = F.softmax(attention_scores, dim=-1) @ V\n",
        "```\n",
        "\n",
        "Obliczamy softmax z $( \\text{attention\\_scores} )$ w celu uzyskania prawdopodobieństw, które określają „na co” dany token zwraca uwagę. Następnie obliczamy „ważoną sumę” wartości $V$ na podstawie macierzy uwagi. Wynikiem jest nowa reprezentacja każdego tokena, wzbogacona o informacje z innych tokenów w oknie.\n",
        "\n",
        "\n",
        "Ze względu na sposób, w jaki ukształtowaliśmy nasze macierze, obliczenia uwagi w oknach są wykonywane wydajnie równolegle w oknach i głowicach uwagi. Na koniec przestawiamy tensory z powrotem na ((h*w),C) i zwracamy nasze ostateczne przewidywane dane wejściowe.\n",
        "\n",
        "![image.png](../images/self-attetention.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp5Je8w4uHEe"
      },
      "source": [
        "Później wprowadzamy **Shifted Window Attention Mechanism** w Swin Transformerach umożliwiający wymianę informacji między nieprzecinającymi się okienkami poprzez wprowadzenie przesunięcia ich układu w kolejnych warstwach. Przesunięcie to sprawia, że sąsiednie okienka częściowo na siebie nachodzą, co pozwala na przepływ informacji przez ich granice. Przesunięcie jest realizowane wydajnie za pomocą operacji cyklicznej (np. `torch.roll`), która przemieszcza okienka o połowę ich rozmiaru.\n",
        "\n",
        "Wyzwanie pojawia się w związku z przesunięciem, ponieważ tokeny z różnych okienek mogą zostać przestrzennie źle dopasowane. Aby temu zapobiec, stosuje się maskowanie uwagi, które blokuje interakcje między tokenami nienależącymi do sąsiednich obszarów obrazu. Maski te są zaprojektowane tak, aby uniemożliwić uwzględnianie informacji między regionami niepołączonymi w oryginalnym układzie.\n",
        "\n",
        "Ten mechanizm nie tylko umożliwia lokalną uwagę w obrębie okienek, ale także wspiera hierarchiczne uczenie cech poprzez tworzenie połączeń między sąsiadującymi okienkami w kolejnych warstwach.\n",
        "\n",
        "![image.png](../images/shifted_window_attention_mechanism.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SxRl44V0uHEe"
      },
      "outputs": [],
      "source": [
        "class ShiftedWindowMSA(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size=7, mask=False, attention_dropout=0.0, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.mask = mask # mask (True/False)\n",
        "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attention_dropout = nn.Dropout(attention_dropout)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n",
        "\n",
        "        self.relative_embeddings = RelativeEmbeddings(window_size, num_heads)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        B, H, W, C = input.shape\n",
        "\n",
        "        # pad feature maps to multiples of window size\n",
        "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
        "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
        "        x = F.pad(input, (0, 0, 0, pad_r, 0, pad_b))\n",
        "        _, pad_H, pad_W, _ = x.shape\n",
        "       \n",
        "        # Cyclic shift\n",
        "        if self.mask:\n",
        "            x = torch.roll(x, (-self.window_size//2, -self.window_size//2), dims=(1,2))\n",
        "\n",
        "        # Partition windows\n",
        "        num_windows = (pad_H //self.window_size) * (pad_W // self.window_size)\n",
        "        x = rearrange(\n",
        "                    x, \n",
        "                    'b (h w_h) (w w_w) c -> (b h w) (w_h w_w) c', \n",
        "                    w_h=self.window_size, w_w=self.window_size\n",
        "                )\n",
        "\n",
        "        # QKV computation\n",
        "        qkv = F.linear(x, self.qkv.weight)\n",
        "        qkv = qkv.reshape(x.size(0), x.size(1), 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Calculate attention \n",
        "        q = q * (C // self.num_heads) ** -0.5\n",
        "        attn = q.matmul(k.transpose(-2, -1))\n",
        "\n",
        "        # Add relative position bias\n",
        "        relative_position_bias = self.relative_embeddings()\n",
        "        attn = attn + relative_position_bias \n",
        "       \n",
        "        if self.mask:\n",
        "            # Create attention mask\n",
        "            attn_mask = torch.zeros((pad_H, pad_W), device=x.device)\n",
        "            \n",
        "            # Generate coordinates for the mask\n",
        "            for i in range(0, pad_H, self.window_size):\n",
        "                for j in range(0, pad_W, self.window_size):\n",
        "                    attn_mask[i:i + self.window_size, j:j + self.window_size] += 1\n",
        "            \n",
        "            # Create mask for each window\n",
        "            attn_mask = rearrange(\n",
        "                attn_mask, \n",
        "                '(h winh) (w winw) -> (h w) (winh winw)', \n",
        "                winh=self.window_size, \n",
        "                winw=self.window_size\n",
        "            )\n",
        "\n",
        "            # Create mask for each window\n",
        "            attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)  # Shape: (num_windows, window_size^2, window_size^2)\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float('-inf')).masked_fill(attn_mask == 0, 0.0)\n",
        "\n",
        "            # Add a dimension for num_heads\n",
        "            attn_mask = attn_mask.unsqueeze(1)  # Shape: (num_windows, 1, window_size^2, window_size^2)\n",
        "\n",
        "            # Broadcast over batch and num_heads\n",
        "            attn = attn.view(-1, num_windows, self.num_heads, x.size(1), x.size(1))\n",
        "            attn = attn + attn_mask.unsqueeze(0)  # Broadcasting over batch and num_heads\n",
        "            attn = attn.view(-1, self.num_heads, x.size(1), x.size(1))\n",
        "\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attention_dropout(attn)\n",
        "\n",
        "        # Attention output\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, -1, C) \n",
        "        x = self.proj(x)\n",
        "        x = self.proj_dropout(x)\n",
        "\n",
        "        # Reverse cyclic shift\n",
        "        x = rearrange(\n",
        "            x, \n",
        "            'b (h ws1 w ws2) c -> b (h ws1) (w ws2) c', \n",
        "            ws1=self.window_size, \n",
        "            ws2=self.window_size,\n",
        "            h = pad_H // self.window_size,\n",
        "            w = pad_W // self.window_size\n",
        "        )\n",
        "        if self.mask: \n",
        "            x = torch.roll(x, (self.window_size//2, self.window_size//2), (1,2))\n",
        "\n",
        "        #unpad features\n",
        "        x = x[:, :H, :W, :].contiguous()\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gtwVHZCuHEe"
      },
      "source": [
        "# Relative Position Embeddings\n",
        "\n",
        "**Relative Position Embeddings**  wprowadzają dodatkową macierz biasu do mechanizmu uwagi własnej, aby uwzględnić relacje przestrzenne między tokenami. W obliczeniach uwagi macierz biasu pozycyjnego $B \\in \\mathbb{R}^{M^2 \\times M^2}$ jest dodawana do wyników podobieństwa, co pozwala modelowi lepiej rozumieć strukturę przestrzenną tokenów w obrębie okienka.\n",
        "\n",
        "Aby zmniejszyć złożoność, pełna macierz $B$ jest generowana z mniejszej macierzy parametrów $\\hat{B} \\in \\mathbb{R}^{(2M-1) \\times (2M-1)}$, gdzie $M$ to rozmiar okienka. Wartości w $B$ są wypełniane na podstawie względnych pozycji tokenów w zakresie $[-M+1, M-1]$ wzdłuż każdej osi. Później macierz $B$ jest dzielona na 4 podmacierze, które są dodawane do wyników podobieństwa w celu uwzględnienia relacji przestrzennych w pionie, poziomie i obu kierunkach przekątnych. Modyfikacja tej formuły wygląda następująco:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V + B\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "W odróżnieniu od osadzeń sinusoidalnych, te osadzenia pozycyjne są uczone podczas treningu, co daje większą elastyczność. Dzięki dodaniu tych osadzeń bezpośrednio do wyników iloczynu zapytań i kluczy, model efektywnie uwzględnia informacje o relacjach przestrzennych, zachowując jednocześnie zgodność wymiarów w obliczeniach uwagi. To rozwiązanie umożliwia lepsze odwzorowanie relacji przestrzennych przy użyciu zwartej reprezentacji parametrów.\n",
        "\n",
        "![image.png](../images/relative_position_embeddings.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VSZ3liW6uHEe"
      },
      "outputs": [],
      "source": [
        "class RelativeEmbeddings(nn.Module):\n",
        "    def __init__(self, window_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size  # Size of the window (e.g., 8x8 or 16x16)\n",
        "        self.num_heads = num_heads  # Number of attention heads\n",
        "\n",
        "        # Initialize relative coordinates and relative position index\n",
        "        self.define_relative_position_bias_table()\n",
        "        self.define_relative_position_index()\n",
        "\n",
        "    def define_relative_position_bias_table(self):\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * self.window_size - 1) * (2 * self.window_size - 1), self.num_heads)\n",
        "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
        "\n",
        "\n",
        "    def define_relative_position_index(self):\n",
        "        \"\"\"\n",
        "        This method defines the relative position index for each pixel pair in the window.\n",
        "        It calculates the differences in positions and generates a unique index for each relative position.\n",
        "        \"\"\"\n",
        "        # Generate coordinates for the height and width of the window\n",
        "        coords_h = torch.arange(self.window_size)\n",
        "        coords_w = torch.arange(self.window_size)\n",
        "\n",
        "        # Create a meshgrid for all the coordinates\n",
        "        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
        "\n",
        "        # Flatten the coordinates into a 2D array\n",
        "        coords_flatten = torch.flatten(coords, 1)\n",
        "\n",
        "        # Calculate the relative position by subtracting each pair of coordinates\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
        "\n",
        "        # Shift the coordinates to ensure positive indices\n",
        "        relative_coords[:, :, 0] += self.window_size - 1\n",
        "        relative_coords[:, :, 1] += self.window_size - 1\n",
        "\n",
        "        # Scale the coordinates to a larger range (for uniqueness)\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
        "\n",
        "        # Sum the two coordinate differences to get a unique index\n",
        "        relative_position_index = relative_coords.sum(-1).flatten()\n",
        "\n",
        "        # Register the relative position index as a buffer to be used during training\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "    def forward(self):\n",
        "      \n",
        "      \n",
        "        # Use the relative position index and the relative coordinates table to compute the bias\n",
        "        relative_position_bias = F.embedding(\n",
        "            self.relative_position_index,  # Look up bias values from the relative position index\n",
        "            self.relative_position_bias_table,  # Use the pre-defined relative position bias table\n",
        "        )\n",
        "\n",
        "        # Reshape the bias values to match the shape of the attention logits (window_size * window_size, window_size * window_size, num_heads)\n",
        "        relative_position_bias = relative_position_bias.view(\n",
        "            self.window_size * self.window_size, self.window_size * self.window_size, self.num_heads\n",
        "        )\n",
        "\n",
        "        # Permute the bias to match the attention mechanism (num_heads, window_size * window_size, window_size * window_size)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().unsqueeze(0)\n",
        "\n",
        "        return relative_position_bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo4toIKsuHEe"
      },
      "source": [
        "# Transformer Encoder Block\n",
        "\n",
        "**Transformer Encoder Block** w Swin Transformer jest zgodny z typową architekturą bloku transformera, z tą różnicą, że wykorzystuje mechanizm uwagi w przesuniętych oknach oraz aktywację GELU w wielowarstwowej perceptronie (MLP). Każdy blok kodera składa się z dwóch głównych etapów: obliczania uwagi oraz przekształceń nieliniowych w MLP.\n",
        "\n",
        "W pierwszym etapie dane wejściowe są normalizowane i przekazywane do mechanizmu uwagi z przesuniętymi oknami (Shifted Window Attention). Mechanizm ten umożliwia komunikację między sąsiednimi oknami, a wynik uwagi jest dodawany jako rezidual do oryginalnych danych.\n",
        "\n",
        "Następnie dane przechodzą przez drugi etap, który obejmuje normalizację, warstwę MLP rozszerzającą wymiar przestrzeni osadzania czterokrotnie, zastosowanie aktywacji GELU oraz powrót do pierwotnego wymiaru. Wynik jest również sumowany z danymi z poprzedniego etapu, co umożliwia lepsze propagowanie informacji w sieci.\n",
        "\n",
        "Dodatkowo wprowadzono klasę **AlternatingEncoderBlock**, która grupuje bloki kodera w pary. Pierwszy blok w parze działa na standardowych oknach, a drugi na przesuniętych oknach, co pozwala na efektywne uchwycenie relacji między tokenami w różnych lokalizacjach.\n",
        "\n",
        "![image.png](../images/transformer_encoder_block.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OAzlYRGAuHEe"
      },
      "outputs": [],
      "source": [
        "class SwinEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size, mask, sd_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.stochastic_depth = StochasticDepth(sd_prob, \"row\") # Stochastic Depth with 0.1 probability of dropping out a row for tiny version of Swin Transformer\n",
        "\n",
        "        self.WMSA = ShiftedWindowMSA(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=mask)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=0.1), # Default dropout probability is 0.0 in the torchvision implementation\n",
        "            nn.Linear(embed_dim*4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Initialization of weights and biases (bias) in linear layers\n",
        "        for m in self.MLP:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight) # Xavier initialization for weights, which prevents the disappearance or explosion of gradients during training.\n",
        "                if m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, std=1e-6) # Set a small offset, to have a small impact in the initial stages of training.\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Attention path with pre-normalization\n",
        "        res1 = x # Save input for the skip connection\n",
        "        x = self.stochastic_depth(self.WMSA(self.layer_norm(x))) # Attention block with LayerNorm and Stochastic Depth(more efficient than Dropout for training transformers)\n",
        "        x = res1 + x # Residual connection\n",
        "\n",
        "        # MLP path with pre-normalization\n",
        "        res2 = x  # Save intermediate result for skip connection\n",
        "        x = self.stochastic_depth(self.MLP(self.layer_norm(x))) # MLP block with LayerNorm and Dropout\n",
        "        x = res2 + x  # Residual connection\n",
        "\n",
        "        return x\n",
        "\n",
        "class AlternatingEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, sd_prob, window_size=7):\n",
        "        super().__init__()\n",
        "        self.WSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=False, sd_prob=sd_prob[0])\n",
        "        self.SWSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=True, sd_prob=sd_prob[1])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.SWSA(self.WSA(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRY6oIyNuHEe"
      },
      "source": [
        "# Final Swin-Transformer Class\n",
        "\n",
        "Mając już zaimplementowane wszystkie komponenty Swin-Transformera, możemy stworzyć jego finalną klasę. Struktura modelu opiera się na oryginalnym artykule, uwzględniając odpowiednie bloki kodera, wymiary osadzeń oraz liczbę głów uwagi.\n",
        "\n",
        "Model zaczyna się od warstwy osadzania (*Embedding Layer*), która przekształca obraz wejściowy w odpowiednią reprezentację. Następnie przechodzi przez cztery etapy obliczeniowe:  \n",
        "1. **Etap 1**: Alternating Encoder Block z 96 wymiarami osadzania i 3 głowami uwagi.  \n",
        "2. **Etap 2**: Alternating Encoder Block z 192 wymiarami osadzania i 6 głowami uwagi.  \n",
        "3. **Etap 3**: Trzy następujące po sobie Alternating Encoder Blocks z 384 wymiarami osadzania i 12 głowami uwagi.  \n",
        "4. **Etap 4**: Alternating Encoder Block z 768 wymiarami osadzania i 24 głowami uwagi.  \n",
        "\n",
        "Każdy etap zawiera proces *Patch Merging*, który zmniejsza rozdzielczość przestrzenną danych i zwiększa liczbę wymiarów kanałów. Finalnie, dane wyjściowe mają wymiary `(1, 49, 768)`, gdzie 1 to wymiar partii, 49 to spłaszczona przestrzeń 7x7, a 768 to liczba kanałów reprezentująca wymiar osadzania.\n",
        "\n",
        "Testując model z obrazem wejściowym o wymiarach `(1, 3, 224, 224)`, możemy potwierdzić, że implementacja działa zgodnie z oczekiwaniami i generuje poprawne dane wyjściowe. Dzięki temu w pełni zaimplementowaliśmy Swin-Transformer w PyTorch od podstaw!\n",
        "\n",
        "![image.png](../images/all_stages_swin.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_CaQahw4uHEe"
      },
      "outputs": [],
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, depth=[2, 2, 6, 2], embed_dim=96, stochastic_depth_prob=0.2, window_size= 7):\n",
        "        super().__init__()\n",
        "        self.Embedding = SwinEmbedding()  # Embedding layer\n",
        "\n",
        "        # Calculate total number of blocks\n",
        "        total_stage_blocks = sum(depth)\n",
        "        stage_block_id = 0\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "\n",
        "        in_channels = embed_dim\n",
        "        for i_stage, num_blocks in enumerate(depth):\n",
        "            temp_sd_prob = []\n",
        "            for _ in range(num_blocks):\n",
        "                # Calculate probability for the current layer\n",
        "                sd_prob = stochastic_depth_prob * float(stage_block_id) / (total_stage_blocks - 1)\n",
        "                temp_sd_prob.append(sd_prob)\n",
        "                stage_block_id += 1\n",
        "\n",
        "            #Add alternating encoder blocks recording to the depth list divided by 2, because each block has 2 sub-blocks\n",
        "            sd_prob = [temp_sd_prob[i:i+2] for i in range(0, len(temp_sd_prob), 2)]\n",
        "            for _ in range(int(num_blocks / 2)):\n",
        "                num_heads = in_channels // 32\n",
        "                #print(f\"AlternatingEncoderBlock({in_channels}, {num_heads}, {sd_prob[0]})\") # Debug\n",
        "                self.stages.append(\n",
        "                    AlternatingEncoderBlock(in_channels, num_heads, sd_prob[0], window_size=window_size)\n",
        "                )\n",
        "                sd_prob.pop(0)\n",
        "\n",
        "            # Add patch merging layer if this is not the last stage\n",
        "            if i_stage < len(depth) - 1:\n",
        "                self.stages.append(PatchMerging(in_channels))\n",
        "                #print(f\"PatchMerging({in_channels})\") # Debug\n",
        "                in_channels *= 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Embedding(x)\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "daBqHcxj32s3"
      },
      "outputs": [],
      "source": [
        "class SwinTransformerMultiStage(nn.Module):\n",
        "    \"\"\"\n",
        "    Subclass (or replacement) of your SwinTransformer that returns\n",
        "    4 feature maps from each stage: C2, C3, C4, C5.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_swin):\n",
        "        super().__init__()\n",
        "        # Copy over the embedding\n",
        "        self.Embedding = base_swin.Embedding\n",
        "        # Copy over the entire 'stages' ModuleList\n",
        "        self.stages = base_swin.stages\n",
        "        # You already know embed_dim=96 for tiny model, but not strictly needed here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1) Patch embedding\n",
        "        x = self.Embedding(x)  # (B, 56*56, 96)\n",
        "\n",
        "        # -- Stage 1\n",
        "        x = self.stages[0](x)\n",
        "        c2 = x\n",
        "        x = self.stages[1](x)\n",
        "\n",
        "        # -- Stage 2\n",
        "        x = self.stages[2](x)\n",
        "        c3 = x\n",
        "        x = self.stages[3](x)\n",
        "\n",
        "        # -- Stage 3\n",
        "        x = self.stages[4](x)\n",
        "        x = self.stages[5](x)\n",
        "        x = self.stages[6](x)\n",
        "        c4 = x\n",
        "        x = self.stages[7](x)\n",
        "\n",
        "        # -- Stage 4\n",
        "        x = self.stages[8](x)\n",
        "        c5 = x\n",
        "        \n",
        "    \n",
        "        # Return all 4 feature maps, C2, C3, C4, C5 convert to (B, C, H, W)\n",
        "        stage_dict = {\n",
        "            \"c2\": rearrange(c2, 'B h w c -> B c h w'),\n",
        "            \"c3\": rearrange(c3, 'B h w c -> B c h w'),\n",
        "            \"c4\": rearrange(c4, 'B h w c -> B c h w'),\n",
        "            \"c5\": rearrange(c5, 'B h w c -> B c h w'),\n",
        "        }\n",
        "      \n",
        "\n",
        "        return stage_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dUQMM_Ar4aE5"
      },
      "outputs": [],
      "source": [
        "class SwinFPNBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    1) Runs the Swin stages -> returns c2..c5\n",
        "    2) Feeds them into a standard FeaturePyramidNetwork -> returns multi-scale feature maps\n",
        "    3) That final dict is what Mask R-CNN expects\n",
        "    \"\"\"\n",
        "    def __init__(self, swin_multistage: nn.Module):\n",
        "        super().__init__()\n",
        "        self.swin = swin_multistage\n",
        "        # Suppose we output 256 channels from FPN\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=[96, 192, 384, 768],  # channels in c2..c5\n",
        "            out_channels=256,\n",
        "            # extra_blocks=LastLevelMaxPool()  # optional\n",
        "        )\n",
        "        self.out_channels = 256  # FPN’s output channels per scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is (B,3,H,W)\n",
        "        # 1) Get raw stage features\n",
        "        features = self.swin(x)  # e.g. {\"c2\":(B,96,56,56), \"c3\":(B,192,28,28), \"c4\":(B,384,14,14), \"c5\":(B,768,7,7)}\n",
        "\n",
        "        # 2) Rename them to match FPN’s expected keys: \"0\", \"1\", \"2\", \"3\" or something\n",
        "        #    or you can pass them in as a dict with the same keys but then set in_channels_list accordingly\n",
        "        fpn_input = {\n",
        "        \"0\": features[\"c2\"],\n",
        "        \"1\": features[\"c3\"],\n",
        "        \"2\": features[\"c4\"],\n",
        "        \"3\": features[\"c5\"],\n",
        "         }\n",
        "\n",
        "        # 3) Run FPN\n",
        "        #    This returns a dict of feature maps at different scales (e.g. \"res2\", \"res3\", \"res4\", \"res5\")\n",
        "        #    each will have shape (B, 256, H_out, W_out)\n",
        "        out = self.fpn(fpn_input)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OreAmIUM4EhC"
      },
      "outputs": [],
      "source": [
        "def build_swin_maskrcnn(num_classes=2, weights_path=None, device=\"cpu\"):\n",
        "    base_swin = SwinTransformer(depth=[2, 2, 6, 2], embed_dim=96, window_size=7) # Tiny Swin Transformer\n",
        "\n",
        "    # Load weights if available\n",
        "    if weights_path is not None:\n",
        "        print(f\"Download from {weights_path}\")\n",
        "        weights = torch.load(weights_path, map_location=device)\n",
        "        if \"model\" in weights:  \n",
        "            weights = weights[\"model\"]\n",
        "        try:\n",
        "            base_swin.load_state_dict(weights, strict=False)\n",
        "            print(\"Weights loaded successfully\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error loading weights: {e}\")\n",
        "\n",
        "    # Convert it to multi-stage\n",
        "    multi_stage_swin = SwinTransformerMultiStage(base_swin)\n",
        "    # Wrap in FPN\n",
        "    backbone = SwinFPNBackbone(multi_stage_swin)\n",
        "\n",
        "    # For multi-scale anchors\n",
        "    anchor_generator = anchor_generator = AnchorGenerator(\n",
        "    sizes=((32,), (64,), (128,), (256,)),  # 4 \"levels\"\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)*4     # or explicitly write 4 tuples\n",
        ")\n",
        "\n",
        "\n",
        "    transform = GeneralizedRCNNTransform(\n",
        "        min_size=(480, 800), # Multi-scale training: short side 480 to 800\n",
        "        max_size=1333, # Long side no more than 1333\n",
        "        image_mean=[0.485, 0.456, 0.406], # Average values updated (ImageNet)\n",
        "        image_std=[0.229, 0.224, 0.225], # Updated std (ImageNet)\n",
        "    )\n",
        "\n",
        "    model = MaskRCNN(\n",
        "        backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_detections_per_img=100,\n",
        "        image_mean=None,\n",
        "        image_std=None,\n",
        "        transform=transform\n",
        "    )\n",
        "    # Force it in case older torchvision\n",
        "    model.transform = transform\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ9x9vOl4RIO",
        "outputId": "9da64df5-aa3b-4aaf-e82b-593f50d7d615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss_classifier': tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>), 'loss_mask': tensor(6.4601, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.6744, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0229, device='cuda:0', grad_fn=<DivBackward0>)}\n",
            "[{'boxes': tensor([[2.0768e+02, 1.2309e+02, 2.1523e+02, 1.3143e+02],\n",
            "        [1.8443e+02, 1.5539e+02, 1.9476e+02, 1.6328e+02],\n",
            "        [9.1385e+01, 8.6958e+01, 9.9394e+01, 9.6394e+01],\n",
            "        [1.0724e+02, 1.5358e+02, 1.1529e+02, 1.6176e+02],\n",
            "        [8.3207e+01, 5.1730e+01, 9.1615e+01, 6.1709e+01],\n",
            "        [3.5698e+01, 1.8069e+02, 4.3570e+01, 1.8990e+02],\n",
            "        [1.9081e+01, 1.3002e+02, 4.4486e+01, 1.4068e+02],\n",
            "        [2.9698e+01, 2.1412e+02, 3.8727e+01, 2.2248e+02],\n",
            "        [1.3694e+02, 2.8885e+01, 1.4669e+02, 3.8957e+01],\n",
            "        [1.8883e+02, 1.5329e+02, 1.9826e+02, 1.6128e+02],\n",
            "        [2.0930e+02, 1.1767e+02, 2.1914e+02, 1.2585e+02],\n",
            "        [2.1490e+02, 1.8531e+02, 2.2376e+02, 1.9156e+02],\n",
            "        [1.4541e+02, 1.7328e+02, 1.5446e+02, 1.8236e+02],\n",
            "        [1.2060e+02, 1.6704e+01, 1.2983e+02, 2.4972e+01],\n",
            "        [1.2313e+02, 2.4759e+01, 1.3193e+02, 3.3376e+01],\n",
            "        [8.8639e+01, 9.0705e+01, 9.7600e+01, 9.9020e+01],\n",
            "        [5.9161e+01, 9.1952e+01, 6.9112e+01, 1.0053e+02],\n",
            "        [1.7836e+02, 1.8784e+02, 1.8792e+02, 1.9649e+02],\n",
            "        [8.1694e+01, 9.1843e+01, 9.0477e+01, 1.0231e+02],\n",
            "        [8.8007e+01, 3.7006e+01, 9.8020e+01, 4.5970e+01],\n",
            "        [1.2137e+02, 1.3839e+01, 1.3274e+02, 2.1088e+01],\n",
            "        [4.2987e-01, 1.0110e+01, 9.1706e+00, 1.9332e+01],\n",
            "        [2.0415e+02, 2.6062e+01, 2.1315e+02, 3.3906e+01],\n",
            "        [8.1166e-02, 3.3011e+01, 6.1823e+00, 4.3557e+01],\n",
            "        [9.2297e+01, 1.5792e+02, 9.9749e+01, 1.6850e+02],\n",
            "        [1.4916e+02, 1.9804e+02, 1.5865e+02, 2.0691e+02],\n",
            "        [2.0303e+01, 1.4139e+02, 2.9792e+01, 1.5036e+02],\n",
            "        [1.5096e+02, 5.8509e+00, 1.7825e+02, 1.7397e+01],\n",
            "        [5.3160e+00, 1.9568e+02, 1.4582e+01, 2.0481e+02],\n",
            "        [7.9708e+01, 9.1723e+01, 8.6894e+01, 1.0101e+02],\n",
            "        [1.9962e+02, 1.0964e+02, 2.0823e+02, 1.1944e+02],\n",
            "        [3.3558e-01, 1.5455e+02, 1.0095e+01, 1.6377e+02],\n",
            "        [1.7113e+02, 7.1547e+01, 1.7965e+02, 7.9501e+01],\n",
            "        [3.1150e-01, 1.8234e+02, 9.4731e+00, 1.9202e+02],\n",
            "        [4.0312e+01, 8.5744e+01, 5.0716e+01, 9.6090e+01],\n",
            "        [1.2518e+02, 1.1678e+01, 1.3419e+02, 2.0801e+01],\n",
            "        [7.3495e+01, 1.8176e+02, 8.3001e+01, 1.9160e+02],\n",
            "        [9.9157e+01, 9.4770e+01, 1.2435e+02, 1.0665e+02],\n",
            "        [9.1813e+01, 9.0735e+01, 1.0036e+02, 9.9232e+01],\n",
            "        [6.9589e+01, 1.1288e+01, 7.9171e+01, 1.9791e+01],\n",
            "        [1.5773e+02, 3.7449e+01, 1.6615e+02, 4.7026e+01],\n",
            "        [6.3750e+01, 9.2742e+01, 7.4162e+01, 1.0131e+02],\n",
            "        [4.4809e-02, 9.5282e+01, 5.8473e+00, 1.0414e+02],\n",
            "        [8.1216e+01, 1.1252e+02, 9.0177e+01, 1.2157e+02],\n",
            "        [1.8679e+02, 1.2937e+02, 1.9722e+02, 1.3859e+02],\n",
            "        [5.5460e+01, 1.9209e+02, 6.4678e+01, 2.0143e+02],\n",
            "        [1.5202e+02, 1.3224e+02, 1.6092e+02, 1.4022e+02],\n",
            "        [4.5422e+00, 6.1687e+01, 1.3760e+01, 7.0524e+01],\n",
            "        [1.2410e+01, 1.3337e+02, 3.6460e+01, 1.4444e+02],\n",
            "        [1.6238e+02, 3.4136e+01, 1.7118e+02, 4.3296e+01],\n",
            "        [1.3734e+02, 1.7832e+02, 1.4581e+02, 1.8795e+02],\n",
            "        [1.2129e+02, 8.4574e+00, 1.3085e+02, 1.7502e+01],\n",
            "        [1.6645e+02, 5.4233e+01, 1.9143e+02, 6.4766e+01],\n",
            "        [1.3277e+02, 7.8393e+01, 1.4157e+02, 8.8994e+01],\n",
            "        [8.5932e+01, 3.5015e+01, 9.5715e+01, 4.3592e+01],\n",
            "        [0.0000e+00, 9.5678e+01, 2.4875e+01, 1.0899e+02],\n",
            "        [1.5295e+02, 1.8761e+02, 1.6207e+02, 1.9668e+02],\n",
            "        [1.6777e+02, 2.1295e+01, 1.7750e+02, 3.1121e+01],\n",
            "        [2.0481e+02, 1.3683e+02, 2.1312e+02, 1.4629e+02],\n",
            "        [7.7825e+01, 4.1425e+01, 1.0505e+02, 5.2093e+01],\n",
            "        [1.5641e+02, 8.4282e+01, 1.6601e+02, 9.3877e+01],\n",
            "        [1.2588e+01, 1.1055e+02, 2.1531e+01, 1.1946e+02],\n",
            "        [7.4052e+00, 3.5409e+01, 3.7668e+01, 4.5932e+01],\n",
            "        [1.9384e+02, 2.5533e+01, 2.1855e+02, 3.7631e+01],\n",
            "        [6.3967e+00, 1.2229e+02, 1.7813e+01, 1.3072e+02],\n",
            "        [2.6968e+01, 2.7340e+01, 3.6228e+01, 3.4561e+01],\n",
            "        [2.0887e+02, 1.0954e+01, 2.1714e+02, 1.9045e+01],\n",
            "        [1.2592e+02, 1.2346e+02, 1.3608e+02, 1.3426e+02],\n",
            "        [2.1241e+02, 1.2868e+02, 2.2181e+02, 1.3670e+02],\n",
            "        [8.8565e+01, 5.1724e+01, 9.9410e+01, 6.0188e+01],\n",
            "        [1.2109e+02, 6.2665e+01, 1.3027e+02, 7.1898e+01],\n",
            "        [4.6423e+01, 1.6463e+01, 6.8595e+01, 2.7628e+01],\n",
            "        [1.5417e+02, 5.5006e+00, 1.6521e+02, 1.3132e+01],\n",
            "        [1.6107e+02, 8.6865e+01, 1.7059e+02, 9.5214e+01],\n",
            "        [1.0152e+02, 8.9899e+01, 1.2440e+02, 1.0145e+02],\n",
            "        [7.5532e+01, 1.6090e+02, 8.3714e+01, 1.6859e+02],\n",
            "        [1.8695e+01, 1.3831e+02, 2.7923e+01, 1.4713e+02],\n",
            "        [9.1220e+01, 4.5251e+01, 1.0225e+02, 5.4088e+01],\n",
            "        [1.6066e+02, 1.7897e+02, 1.6944e+02, 1.8879e+02],\n",
            "        [9.9065e+01, 1.7104e+02, 1.0790e+02, 1.7986e+02],\n",
            "        [9.8931e+01, 5.2241e+01, 1.2513e+02, 6.3476e+01],\n",
            "        [9.2150e+01, 3.3303e+01, 1.1546e+02, 4.4779e+01],\n",
            "        [9.0202e+01, 5.6612e+01, 9.9944e+01, 6.5334e+01],\n",
            "        [5.2272e+01, 4.6535e+00, 6.3119e+01, 1.3107e+01],\n",
            "        [1.2536e+02, 9.0239e+01, 1.3443e+02, 9.8827e+01],\n",
            "        [6.3452e+01, 1.6906e+02, 7.1500e+01, 1.7704e+02],\n",
            "        [6.9677e+01, 2.1763e+02, 7.7875e+01, 2.2368e+02],\n",
            "        [8.6245e+00, 1.0277e+02, 1.8138e+01, 1.1364e+02],\n",
            "        [2.8267e+01, 2.8301e+01, 5.1720e+01, 3.8682e+01],\n",
            "        [7.2449e+01, 1.1459e+02, 8.2849e+01, 1.2373e+02],\n",
            "        [8.3431e+01, 5.9942e+01, 9.2605e+01, 6.8172e+01],\n",
            "        [4.1594e+01, 1.1530e+02, 6.6160e+01, 1.2633e+02],\n",
            "        [1.6978e+02, 1.9232e+02, 1.7947e+02, 2.0089e+02],\n",
            "        [5.9244e+01, 6.6281e+01, 6.7811e+01, 7.5553e+01],\n",
            "        [9.3793e+01, 1.8598e+02, 1.0298e+02, 1.9600e+02],\n",
            "        [0.0000e+00, 1.5003e+02, 4.6436e+00, 1.5864e+02],\n",
            "        [1.2581e+02, 1.4154e+02, 1.5330e+02, 1.5323e+02],\n",
            "        [1.3657e+02, 1.8241e+02, 1.4599e+02, 1.8951e+02],\n",
            "        [1.2881e+01, 1.9486e+02, 3.8749e+01, 2.0805e+02],\n",
            "        [1.7342e+02, 6.2493e+01, 1.8211e+02, 7.2238e+01]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.5799, 0.5771, 0.5752, 0.5739, 0.5719, 0.5674, 0.5664, 0.5648, 0.5633,\n",
            "        0.5613, 0.5592, 0.5582, 0.5573, 0.5560, 0.5537, 0.5517, 0.5517, 0.5517,\n",
            "        0.5512, 0.5465, 0.5441, 0.5393, 0.5385, 0.5384, 0.5374, 0.5363, 0.5354,\n",
            "        0.5346, 0.5336, 0.5326, 0.5319, 0.5314, 0.5309, 0.5309, 0.5284, 0.5281,\n",
            "        0.5279, 0.5277, 0.5272, 0.5267, 0.5260, 0.5249, 0.5242, 0.5241, 0.5240,\n",
            "        0.5237, 0.5233, 0.5214, 0.5205, 0.5202, 0.5199, 0.5198, 0.5186, 0.5184,\n",
            "        0.5169, 0.5167, 0.5164, 0.5161, 0.5160, 0.5154, 0.5152, 0.5152, 0.5150,\n",
            "        0.5150, 0.5147, 0.5147, 0.5144, 0.5140, 0.5139, 0.5134, 0.5133, 0.5130,\n",
            "        0.5128, 0.5126, 0.5121, 0.5117, 0.5116, 0.5114, 0.5111, 0.5108, 0.5108,\n",
            "        0.5106, 0.5101, 0.5101, 0.5094, 0.5092, 0.5090, 0.5086, 0.5085, 0.5084,\n",
            "        0.5084, 0.5079, 0.5078, 0.5071, 0.5071, 0.5069, 0.5069, 0.5067, 0.5063,\n",
            "        0.5062], device='cuda:0'), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}]\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = build_swin_maskrcnn(num_classes=2, device=device).to(device)\n",
        "    x = [torch.randn(3, 224, 224, device=device)]\n",
        "    targets = [{\n",
        "        \"boxes\": torch.tensor([[50,50,150,150]], dtype=torch.float32, device=device),\n",
        "        \"labels\": torch.tensor([1], device=device),\n",
        "        \"masks\": torch.randint(0,2,(1,224,224), device=device, dtype=torch.uint8),\n",
        "    }]\n",
        "\n",
        "    model.train()\n",
        "    losses = model(x, targets)  # forward pass -> dict of losses\n",
        "    print(losses)  # e.g. { 'loss_classifier':..., 'loss_box_reg':..., ... }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(x)  # inference\n",
        "        print(preds)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# code for training the model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from lightning import LightningModule\n",
        "\n",
        "# <-- Import or define your build_swin_maskrcnn function here\n",
        "# from your_swin_file import build_swin_maskrcnn\n",
        "\n",
        "\n",
        "class SwinMaskRCNNModule(LightningModule):\n",
        "    def __init__(self, num_classes=2, lr=1e-4, weights_path=None, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        num_classes: # of classes (including background). \n",
        "                     If you have 1 actual class, use num_classes=2 \n",
        "                     (class + background).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = build_swin_maskrcnn(num_classes=num_classes, weights_path=weights_path, device=device)\n",
        "        self.lr = lr\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Lightning’s hook for a single training batch.\n",
        "\n",
        "        batch = (images, targets)\n",
        "            - images: list of Tensors [C,H,W]\n",
        "            - targets: list of dicts { 'boxes', 'labels', 'masks', etc. }\n",
        "        \"\"\"\n",
        "        images, targets = batch\n",
        "        # Forward pass in Mask R-CNN returns a dict of losses in training mode\n",
        "        loss_dict = self.model(images, targets)\n",
        "        total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Log the total loss\n",
        "        self.log(\"train_loss\", total_loss, prog_bar=True)\n",
        "        return total_loss\n",
        "\n",
        "    # def validation_step(self, batch, CosineAnnealingLRbatch_idx):\n",
        "    #     \"\"\"\n",
        "    #     For validation, Mask R-CNN still returns losses if we pass targets.\n",
        "    #     \"\"\"\n",
        "    #     images, targets = batch\n",
        "    #     loss_dict = self.model(images, targets)\n",
        "    #     total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "    #     self.log(\"val_loss\", total_loss, prog_bar=True)\n",
        "    #     return total_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "\n",
        "        # Force the model into training mode so it *will* return losses\n",
        "        self.model.train()\n",
        "        with torch.no_grad():\n",
        "            loss_dict = self.model(images, targets)\n",
        "\n",
        "        total_loss = sum(loss for loss in loss_dict.values())\n",
        "        self.log(\"val_loss\", total_loss, prog_bar=True)\n",
        "\n",
        "        # Optionally switch back\n",
        "        self.model.eval()\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
        "        #scheduler = CosineAnnealingLR(optimizer, T_max=36, eta_min=self.lr * 0.1)\n",
        "\n",
        "        return optimizer #, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.v2 as v2\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "\n",
        "class CocoDetectionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    COCO-format dataset returning (image, target) pairs for Mask R-CNN.\n",
        "    Each target is a dict:\n",
        "        {\n",
        "            \"boxes\": FloatTensor (N,4),\n",
        "            \"labels\": Int64Tensor (N,),\n",
        "            \"masks\": UInt8Tensor (N,H,W),\n",
        "            \"image_id\": IntTensor (1,)\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_dir,         # e.g. BASE_DIR / \"dataset/coco10/train2017_subset/images\"\n",
        "        # e.g. BASE_DIR / \"dataset/coco10/train2017_subset/coco10_train_annotations.json\"\n",
        "        ann_file,\n",
        "        transforms=None,\n",
        "        single_class=False  # Set True to ignore COCO category_id's and treat as one class\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_dir = str(image_dir)\n",
        "        self.coco = COCO(str(ann_file))\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        self.single_class = single_class\n",
        "\n",
        "        # Example basic transform pipeline\n",
        "        # (You can do advanced augmentations with v2.* or Albumentations)\n",
        "        self._transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Load image\n",
        "        img_info = self.coco.imgs[img_id]\n",
        "        path = img_info['file_name']\n",
        "        img_path = os.path.join(self.image_dir, path)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Build up lists of bounding boxes, masks, labels\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "\n",
        "        for ann in anns:\n",
        "            # Convert [x, y, w, h] to [x_min, y_min, x_max, y_max]\n",
        "            x, y, w, h = ann['bbox']\n",
        "            x2 = x + w\n",
        "            y2 = y + h\n",
        "            boxes.append([x, y, x2, y2])\n",
        "\n",
        "            # Single-class or actual category_id\n",
        "            if self.single_class:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                labels.append(ann[\"category_id\"])\n",
        "\n",
        "            # Build per-object binary mask\n",
        "            m = self.coco.annToMask(ann)  # shape: (H, W)\n",
        "            masks.append(m)\n",
        "\n",
        "        # If no annotations, create dummy\n",
        "        if len(boxes) == 0:\n",
        "            boxes = np.zeros((0, 4), dtype=np.float32)\n",
        "            labels = np.zeros((0,), dtype=np.int64)\n",
        "            masks = np.zeros((0, img.height, img.width), dtype=np.uint8)\n",
        "        else:\n",
        "            boxes = np.array(boxes, dtype=np.float32)\n",
        "            labels = np.array(labels, dtype=np.int64)\n",
        "            masks = np.stack(masks, axis=0).astype(np.uint8)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target[\"masks\"] = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        target[\"image_id\"] = torch.tensor([img_id], dtype=torch.int64)\n",
        "\n",
        "        if self._transforms:\n",
        "            # apply the transforms\n",
        "            img, target = self._transforms(img, target)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_transforms():\n",
        "    return v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
        "        v2.RandomHorizontalFlip(p=0.5),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                     std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_val_transforms():\n",
        "    return v2.Compose([\n",
        "        v2.ToImage(),\n",
        "        v2.Resize(size=(224, 224), antialias=True),\n",
        "        v2.ToDtype(torch.float32, scale=True),\n",
        "        v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                     std=[0.229, 0.224, 0.225]),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/wladyka/Swin-Transformer\n",
            "loading annotations into memory...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done (t=0.92s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "Download from /home/wladyka/Swin-Transformer/swin_tiny_patch4_window7_224.pth\n",
            "Weights loaded successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1761799/4294303689.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights = torch.load(weights_path, map_location=device)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f24d6d02690>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:944\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[0;32m--> 944\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_setup_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001b[39;00m\n\u001b[1;32m    945\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: configuring model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:96\u001b[0m, in \u001b[0;36m_call_setup_hook\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m logger \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mloggers:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     97\u001b[0m         _ \u001b[38;5;241m=\u001b[39m logger\u001b[38;5;241m.\u001b[39mexperiment\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/fabric/loggers/logger.py:118\u001b[0m, in \u001b[0;36mrank_zero_experiment.<locals>.experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _DummyExperiment()\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/pytorch/loggers/mlflow.py:180\u001b[0m, in \u001b[0;36mMLFlowLogger.experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     expt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mlflow_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m expt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/mlflow/tracking/client.py:1287\u001b[0m, in \u001b[0;36mMlflowClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve an experiment by experiment name from the backend store\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03m    Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/mlflow/tracking/_tracking_service/client.py:502\u001b[0m, in \u001b[0;36mTrackingServiceClient.get_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;124;03m    name: The experiment name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;124;03m    :py:class:`mlflow.entities.Experiment`\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:522\u001b[0m, in \u001b[0;36mRestStore.get_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    521\u001b[0m req_body \u001b[38;5;241m=\u001b[39m message_to_json(GetExperimentByName(experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name))\n\u001b[0;32m--> 522\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetExperimentByName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Experiment\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mexperiment)\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:82\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint)\u001b[0m\n\u001b[1;32m     81\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:374\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[0m\n\u001b[1;32m    373\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m--> 374\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:181\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_http_response_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_jitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost_creds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m to:\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[38;5;241m=\u001b[39m env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    842\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    843\u001b[0m )\n\u001b[0;32m--> 844\u001b[0m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/util/retry.py:363\u001b[0m, in \u001b[0;36mRetry.sleep\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sleep_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/urllib3/util/retry.py:347\u001b[0m, in \u001b[0;36mRetry._sleep_backoff\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 102\u001b[0m\n\u001b[1;32m     94\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     95\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     96\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m     logger\u001b[38;5;241m=\u001b[39mmlf_logger,\n\u001b[1;32m     98\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback, early_stop_callback]\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 7) Fit\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Swin-Transformer/venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ],
      "source": [
        "from lightning import seed_everything\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import lightning as pl\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from lightning.pytorch.loggers import MLFlowLogger\n",
        "\n",
        "# Re-use your BASE_DIR logic if you like\n",
        "os.chdir(\"..\")\n",
        "BASE_DIR = Path(os.getcwd()).resolve()\n",
        "print(BASE_DIR)\n",
        "# 1) Create the train/val dataset\n",
        "train_image_dir = BASE_DIR / \"dataset/coco10/train2017_subset/images\"\n",
        "train_ann_file = BASE_DIR / \\\n",
        "    \"dataset/coco10/train2017_subset/coco10_train_annotations.json\"\n",
        "\n",
        "val_image_dir = BASE_DIR / \"dataset/coco10/val2017_subset/images\"\n",
        "val_ann_file = BASE_DIR / \"dataset/coco10/val2017_subset/coco10_val_annotations.json\"\n",
        "\n",
        "url_swin_t = \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth\"\n",
        "weight_path = BASE_DIR / \"swin_tiny_patch4_window7_224.pth\"\n",
        "#Load weights if weights_path does not exist\n",
        "if not weight_path.exists():\n",
        "    print(f\"Downloading weights from {url_swin_t}\")\n",
        "    r = requests.get(url_swin_t)\n",
        "    with open(weight_path, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "train_dataset = CocoDetectionDataset(\n",
        "    image_dir=train_image_dir,\n",
        "    ann_file=train_ann_file,\n",
        "    transforms=get_train_transforms(),\n",
        "    single_class=False   # or True if you want a single-class approach\n",
        ")\n",
        "\n",
        "val_dataset = CocoDetectionDataset(\n",
        "    image_dir=val_image_dir,\n",
        "    ann_file=val_ann_file,\n",
        "    transforms=get_val_transforms(),\n",
        "    single_class=False\n",
        ")\n",
        "\n",
        "# 2) DataLoaders\n",
        "# Mask R-CNN expects a list of images & targets, so we need a custom collate:\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,  # pick what fits your GPU\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# 3) Instantiate our LightningModule (Swin + MaskRCNN)\n",
        "seed_everything(42)\n",
        "\n",
        "model = SwinMaskRCNNModule(num_classes=26, lr=1e-4, weights_path=weight_path, device=\"cuda\")\n",
        "\n",
        "# 4) MLFlow Logger\n",
        "mlf_logger = MLFlowLogger(\n",
        "    experiment_name=\"swin_maskrcnn_experiment\",\n",
        "    tracking_uri=\"http://localhost:5000\",\n",
        "    log_model=True\n",
        ")\n",
        "\n",
        "# 5) (Optional) Callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    save_top_k=1,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    dirpath=\"checkpoints/\",\n",
        "    filename=\"swinmaskrcnn-{epoch:02d}-{val_loss:.4f}\"\n",
        ")\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "# 6) Trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    logger=mlf_logger,\n",
        "    callbacks=[checkpoint_callback, early_stop_callback]\n",
        ")\n",
        "\n",
        "# 7) Fit\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "studia-UG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
