{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDrdNNEUI5aE"
      },
      "source": [
        "# Imports\n",
        "\n",
        "* Importy PyTorch: torch, torch.nn i torch.nn.functional używane do podstawowych operacji tensorowych i modułów sieci neuronowych.\n",
        "* Import math służy do normalizacji pierwiastka kwadratowego w attention.\n",
        "* Einops rearrange służy do przekształcania i permutacji tensorów w przyjazny dla czytelnika i wydajny sposób.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "5EUd4fk8Izsy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from einops import rearrange\n",
        "from torchvision.ops.stochastic_depth import StochasticDepth # Add stochastic depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUEtrxD52Qv8"
      },
      "source": [
        "# Patch Partition + Linear Embedding\n",
        "\n",
        "---\n",
        "„Najpierw dzieli wejściowy obraz RGB na nienakładające się patchs za pomocą modułu dzielenia patch, takiego jak ViT. Każda patch jest traktowana jako „token”, a jej cecha jest ustawiana jako konkatenacja surowych wartości RGB pikseli. W naszej implementacji używamy patch o rozmiarze 4 × 4, a zatem wymiar funkcji każdego patcha wynosi 4 × 4 × 3 = 48. Liniowa warstwa osadzania jest stosowana na tej surowej funkcji, aby rzutować ją na dowolny wymiar (oznaczony jako C)”.\n",
        "\n",
        "---\n",
        "\n",
        "Gdzie C jest hyperparametrem, który określa wymiar osadzenia. W naszym przypadku C = 96, dla modelu Swin-Transformer(tiny).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![image](../images/Patch_Partition_Linear_Embedding.png)\n",
        "\n",
        "\n",
        "Podział patchy w stylu ViT i liniowe embeding można zrealizować za pomocą splotu z rozmiarem jądra, krokiem (stride) równym rozmiarowi patcha oraz wyjściowymi kanałami równymi \\(C\\). Wynikowy tensor ma wymiary \\(H/p * W/p * C\\), gdzie każdy „token” odpowiada liniowemu przekształceniu pikseli patcha. Wymiar embedings \\(C\\) to liczba cech (kanałów), które opisują każdą jednostkę w reprezentacji danych. W naszym przypadku \\(C = 96\\).\n",
        "\n",
        "Klasa **SwinEmbedding**, dziedzicząca z **nn.Module**, inicjalizuje:\n",
        "1. Warstwę splotu \\(p * p\\) (stride \\(p\\)), z kanałami wyjściowymi \\(C\\),\n",
        "2. **LayerNorm** dla wymiaru embeding \\(C\\),\n",
        "3. Funkcję aktywacji ReLU.\n",
        "\n",
        "W metodzie `forward` wejście jest przepuszczane przez splot, przekształcane i permutowane, łącząc \\(H, W\\) w \\(H * W / p^2\\), a wymiar osadzania \\(C\\) przesuwany na końcową pozycję. Na końcu stosowane są normalizacja i ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "pWnJy2ODJ-Ak"
      },
      "outputs": [],
      "source": [
        "class SwinEmbedding(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  input shape -> (b,c,h,w)\n",
        "  output shape -> (b, (h/4 * w/4), C)\n",
        "\n",
        "  Where:\n",
        "\n",
        "  b - batch size\n",
        "  c - number of channels\n",
        "  h - height of the image\n",
        "  w - width of the image\n",
        "  C - number of channels in the output\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, patch_size = 4, C = 96):\n",
        "      super().__init__()\n",
        "      self.linear_embedding = nn.Conv2d(3,C, kernel_size=patch_size, stride=patch_size)\n",
        "      self.layer_norm = nn.LayerNorm(C)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear_embedding(x)\n",
        "    x = rearrange(x, 'b c h w -> b (h w) c')  # spłaszczenie wymiarów przestrzennych obrazu przy pomocy mnożenia h i w\n",
        "    x = self.layer_norm(x) # normalizacja\n",
        "    x = self.relu(x) # funkcja aktywacji (dodanie nieliniowości)\n",
        "\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPtP1bboQqEc"
      },
      "source": [
        "# Patch Merging Layer\n",
        "\n",
        "![image](../images/hearachical_system.png)\n",
        "\n",
        "Aby stworzyć hierarchiczną reprezentację, liczba tokenów jest zmniejszana przez warstwy scalania patchy, gdy sieć staje się głębsza. Pierwsza warstwa scalania patch'y łączy cechy każdej grupy 2 × 2 sąsiednich patch'y i stosuje warstwę liniową na 4C-wymiarowych połączonych cechach. Zmniejsza to liczbę tokenów o wielokrotność 2×2 = 4 (2-krotne zmniejszenie rozdzielczości), a wymiar wyjściowy jest ustawiony na 2C.\n",
        "Inicjalizujemy warstwę liniową z kanałami wejściowymi 4C do kanałów wyjściowych 2C i inicjalizujemy normę warstwy z wyjściowym rozmiarem osadzania. W naszej funkcji forward używamy einops rearrange do zmiany kształtu naszych tokenów z 2x2xC na 1x1x4C. Kończymy, przepuszczając nasze dane wejściowe przez projekcję liniową i normę warstwy.\n",
        "\n",
        "![image](../images/Patch_mergering.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "syDDKgD8MTW0"
      },
      "outputs": [],
      "source": [
        "class PatchMerging(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  Reduces tokens by a factor of 4 (2x2 patches) and doubles embedding dimension.\n",
        "\n",
        "\n",
        "  input shape -> (b (h w) c)\n",
        "  output shape -> (b (h/2 * w/2) C*2)\n",
        "\n",
        "  Where:\n",
        "\n",
        "  b - batch size\n",
        "  c - number of channels\n",
        "  h - height of the image\n",
        "  w - width of the image\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, C) -> None:\n",
        "     super().__init__()\n",
        "     self.linear_layer = nn.Linear(C*4, C*2) # podwajamy wymiar embeddingów\n",
        "     self.layer_norm = nn.LayerNorm(2 * C) # normalizacja\n",
        "\n",
        "  def forward(self, x):\n",
        "    height = width = int(math.sqrt(x.shape[1])/ 2) # obliczamy nową wysokość i szerokość obrazu\n",
        "    x = rearrange(x, 'b (h s1 w s2) c -> b (h w) (s2 s1 c)', s1=2, s2=2, h=height, w=width)\n",
        "    x = self.linear_layer(x)\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfEJmwXVuHEe"
      },
      "source": [
        "# Shifted Window Attention Mechanism\n",
        "\n",
        "\n",
        "Zaczynamy od zainicjowania naszych parametrów embed_dim, num_heads i window_size oraz zdefiniowania dwóch projekcji liniowych. Pierwsza z nich to nasza projekcja z danych wejściowych do zapytań, kluczy i wartości, którą wykonujemy w jednej równoległej projekcji, więc rozmiar wyjściowy jest ustawiony na 3*C. Druga projekcja to projekcja liniowa zastosowana po obliczeniach uwagi. Projekcja ta służy do komunikacji między połączonymi równoległymi wielogłowicowymi jednostkami uwagi.\n",
        "\n",
        "Rozpoczynamy naszą funkcję do przodu, uzyskując rozmiar naszej głowy, wysokość i szerokość naszego wejścia, ponieważ potrzebujemy tych parametrów do zmiany układu. Następnie wykonujemy projekcję Q,K,V na naszym wejściu o kształcie ((h*w), c) do ((h*w), 3C). Nasz następny krok składa się z dwóch części, w których zmienimy nasze dane wejściowe ((h*w), C*3) na okna i równoległe głowice uwagi do naszych obliczeń uwagi.\n",
        "\n",
        "Pózniej rozbijamy naszą macierz na 3 macierze Q,K,V i obliczamy uwagę za pomocą standardowego wzoru uwagi:\n",
        "\n",
        "Formuła self-attention w mechanizmie transformera wygląda następująco:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Obliczanie Attention Scores**\n",
        "\n",
        "```python\n",
        "attention_scores = (Q @ K.transpose(4, 5)) / math.sqrt(h_dim)\n",
        "```\n",
        "\n",
        "Dla każdego tokena obliczamy podobieństwo (iloczyn skalarny) między wektorem zapytania  $Q$ a wszystkimi kluczami $K$. Następnie dzielimy przez $(\\sqrt{d_k})$, aby zachować stabilność gradientów. Pózniej wyniki zmarnalizowane poprzez  dzielienia na $d_k$, gdzie $d_k$ to wymiar wektorów $Q$ i $K$. Dzielimy przez $(\\sqrt{d_k})$, aby zachować stabilność gradientów.\n",
        "\n",
        "\n",
        "**Softmax i kontekst uwagi**\n",
        "\n",
        "```python\n",
        "attention = F.softmax(attention_scores, dim=-1) @ V\n",
        "```\n",
        "\n",
        "Obliczamy softmax z $( \\text{attention\\_scores} )$ w celu uzyskania prawdopodobieństw, które określają „na co” dany token zwraca uwagę. Następnie obliczamy „ważoną sumę” wartości $V$ na podstawie macierzy uwagi. Wynikiem jest nowa reprezentacja każdego tokena, wzbogacona o informacje z innych tokenów w oknie.\n",
        "\n",
        "\n",
        "Ze względu na sposób, w jaki ukształtowaliśmy nasze macierze, obliczenia uwagi w oknach są wykonywane wydajnie równolegle w oknach i głowicach uwagi. Na koniec przestawiamy tensory z powrotem na ((h*w),C) i zwracamy nasze ostateczne przewidywane dane wejściowe.\n",
        "\n",
        "![image.png](../images/self-attetention.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp5Je8w4uHEe"
      },
      "source": [
        "Później wprowadzamy **Shifted Window Attention Mechanism** w Swin Transformerach umożliwiający wymianę informacji między nieprzecinającymi się okienkami poprzez wprowadzenie przesunięcia ich układu w kolejnych warstwach. Przesunięcie to sprawia, że sąsiednie okienka częściowo na siebie nachodzą, co pozwala na przepływ informacji przez ich granice. Przesunięcie jest realizowane wydajnie za pomocą operacji cyklicznej (np. `torch.roll`), która przemieszcza okienka o połowę ich rozmiaru.\n",
        "\n",
        "Wyzwanie pojawia się w związku z przesunięciem, ponieważ tokeny z różnych okienek mogą zostać przestrzennie źle dopasowane. Aby temu zapobiec, stosuje się maskowanie uwagi, które blokuje interakcje między tokenami nienależącymi do sąsiednich obszarów obrazu. Maski te są zaprojektowane tak, aby uniemożliwić uwzględnianie informacji między regionami niepołączonymi w oryginalnym układzie.\n",
        "\n",
        "Ten mechanizm nie tylko umożliwia lokalną uwagę w obrębie okienek, ale także wspiera hierarchiczne uczenie cech poprzez tworzenie połączeń między sąsiadującymi okienkami w kolejnych warstwach.\n",
        "\n",
        "![image.png](../images/shifted_window_attention_mechanism.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "SxRl44V0uHEe"
      },
      "outputs": [],
      "source": [
        "class ShiftedWindowMSA(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    input shape -> (b , (h*w), C)\n",
        "    output shape -> (b , (h*w), C)\n",
        "\n",
        "    Where:\n",
        "\n",
        "    b - batch size\n",
        "    h - height of the image\n",
        "    w - width of the image\n",
        "    C - number of channels in the output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, window_size=7, mask=False):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim # wymiar embeddingów\n",
        "        self.num_heads = num_heads # liczba głów\n",
        "        self.window_size = window_size # rozmiar okna\n",
        "        self.mask = mask # maska (True/False)\n",
        "        self.proj1 = nn.Linear(embed_dim, 3*embed_dim) # projekcja wejścia\n",
        "        self.proj2 = nn.Linear(embed_dim, embed_dim) # projekcja wyjścia\n",
        "        self.embeddings = RelativeEmbeddings()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_dim = self.embed_dim / self.num_heads # obliczamy wymiar pojedynczej głowy\n",
        "        height = width = int(math.sqrt(x.shape[1]))\n",
        "        x = self.proj1(x)\n",
        "        x = rearrange(x, 'b (h w) (c K) -> b h w c K', K=3, h=height, w=width) # zmiana wymiarów, gdzie K to liczba macierzy Q,K,V\n",
        "\n",
        "        if self.mask: # jeśli maska jest True, to wykonujemy przesunięcie okna o połowę\n",
        "            x = torch.roll(x, (-self.window_size//2, -self.window_size//2), dims=(1,2))\n",
        "\n",
        "        # zmiana wymiarów\n",
        "        x = rearrange(x, 'b (h m1) (w m2) (H E) K -> b H h w (m1 m2) E K', H=self.num_heads, m1=self.window_size, m2=self.window_size)\n",
        "\n",
        "        # podział na macierze Q,K,V\n",
        "        Q, K, V = x.chunk(3, dim=6)  # dzielimy na 3 części\n",
        "        Q, K, V = Q.squeeze(-1), K.squeeze(-1), V.squeeze(-1) # usuwamy ostatni wymiar, bo nie jest potrzebny\n",
        "        attention_scores = (Q @ K.transpose(4,5)) / math.sqrt(h_dim) # obliczamy self-attention score\n",
        "        attention_scores = self.embeddings(attention_scores) # dodajemy embeddingsy\n",
        "\n",
        "        '''\n",
        "        H - attention heads\n",
        "        h,w - vertical and horizontal dimensions of the image\n",
        "        (m1 m2) - total size of the window\n",
        "        E - head dimension\n",
        "        K = 3 - constant to break our matrix into 3 Q,K,V matricies\n",
        "\n",
        "        shape of attention_scores = (b, H, h, w, (m1*m2), (m1*m2))\n",
        "        we simply have to generate our row/column masks and apply them\n",
        "        to the last row and columns of windows which are [:,:,-1,:] and [:,:,:,-1]\n",
        "\n",
        "        '''\n",
        "\n",
        "        if self.mask: # jeśli maska jest True, to wykonujemy maskowanie ostatnich wierszy i kolumn w oknie\n",
        "            row_mask = torch.zeros((self.window_size**2, self.window_size**2)).cuda() # tworzymy maskę\n",
        "            row_mask[-self.window_size * (self.window_size//2):, 0:-self.window_size * (self.window_size//2)] = float('-inf')\n",
        "            row_mask[0:-self.window_size * (self.window_size//2), -self.window_size * (self.window_size//2):] = float('-inf')\n",
        "            column_mask = rearrange(row_mask, '(r w1) (c w2) -> (w1 r) (w2 c)', w1=self.window_size, w2=self.window_size).cuda() # maska kolumn\n",
        "            attention_scores[:, :, -1, :] += row_mask\n",
        "            attention_scores[:, :, :, -1] += column_mask\n",
        "\n",
        "        attention = F.softmax(attention_scores, dim=-1) @ V # Softmax i mnożenie przez V\n",
        "        x = rearrange(attention, 'b H h w (m1 m2) E -> b (h m1) (w m2) (H E)', m1=self.window_size, m2=self.window_size)\n",
        "\n",
        "        if self.mask: # Z powrotem przesuwamy okno o połowę\n",
        "            x = torch.roll(x, (self.window_size//2, self.window_size//2), (1,2))\n",
        "\n",
        "        x = rearrange(x, 'b h w c -> b (h w) c')\n",
        "        return self.proj2(x) # projekcja wyjścia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gtwVHZCuHEe"
      },
      "source": [
        "# Relative Position Embeddings\n",
        "\n",
        "**Relative Position Embeddings**  wprowadzają dodatkową macierz biasu do mechanizmu uwagi własnej, aby uwzględnić relacje przestrzenne między tokenami. W obliczeniach uwagi macierz biasu pozycyjnego $B \\in \\mathbb{R}^{M^2 \\times M^2}$ jest dodawana do wyników podobieństwa, co pozwala modelowi lepiej rozumieć strukturę przestrzenną tokenów w obrębie okienka.\n",
        "\n",
        "Aby zmniejszyć złożoność, pełna macierz $B$ jest generowana z mniejszej macierzy parametrów $\\hat{B} \\in \\mathbb{R}^{(2M-1) \\times (2M-1)}$, gdzie $M$ to rozmiar okienka. Wartości w $B$ są wypełniane na podstawie względnych pozycji tokenów w zakresie $[-M+1, M-1]$ wzdłuż każdej osi. Później macierz $B$ jest dzielona na 4 podmacierze, które są dodawane do wyników podobieństwa w celu uwzględnienia relacji przestrzennych w pionie, poziomie i obu kierunkach przekątnych. Modyfikacja tej formuły wygląda następująco:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V + B\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "W odróżnieniu od osadzeń sinusoidalnych, te osadzenia pozycyjne są uczone podczas treningu, co daje większą elastyczność. Dzięki dodaniu tych osadzeń bezpośrednio do wyników iloczynu zapytań i kluczy, model efektywnie uwzględnia informacje o relacjach przestrzennych, zachowując jednocześnie zgodność wymiarów w obliczeniach uwagi. To rozwiązanie umożliwia lepsze odwzorowanie relacji przestrzennych przy użyciu zwartej reprezentacji parametrów.\n",
        "\n",
        "![image.png](../images/relative_position_embeddings.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "VSZ3liW6uHEe"
      },
      "outputs": [],
      "source": [
        "class RelativeEmbeddings(nn.Module):\n",
        "    def __init__(self, window_size=7):\n",
        "        super().__init__()\n",
        "        B = nn.Parameter(torch.randn(2*window_size-1, 2*window_size-1))\n",
        "        x = torch.arange(1,window_size+1,1/window_size)\n",
        "        x = (x[None, :]-x[:, None]).int()\n",
        "        y = torch.concat([torch.arange(1,window_size+1)] * window_size)\n",
        "        y = (y[None, :]-y[:, None])\n",
        "        self.embeddings = nn.Parameter((B[x[:,:], y[:,:]]), requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo4toIKsuHEe"
      },
      "source": [
        "# Transformer Encoder Block\n",
        "\n",
        "**Transformer Encoder Block** w Swin Transformer jest zgodny z typową architekturą bloku transformera, z tą różnicą, że wykorzystuje mechanizm uwagi w przesuniętych oknach oraz aktywację GELU w wielowarstwowej perceptronie (MLP). Każdy blok kodera składa się z dwóch głównych etapów: obliczania uwagi oraz przekształceń nieliniowych w MLP.\n",
        "\n",
        "W pierwszym etapie dane wejściowe są normalizowane i przekazywane do mechanizmu uwagi z przesuniętymi oknami (Shifted Window Attention). Mechanizm ten umożliwia komunikację między sąsiednimi oknami, a wynik uwagi jest dodawany jako rezidual do oryginalnych danych.\n",
        "\n",
        "Następnie dane przechodzą przez drugi etap, który obejmuje normalizację, warstwę MLP rozszerzającą wymiar przestrzeni osadzania czterokrotnie, zastosowanie aktywacji GELU oraz powrót do pierwotnego wymiaru. Wynik jest również sumowany z danymi z poprzedniego etapu, co umożliwia lepsze propagowanie informacji w sieci.\n",
        "\n",
        "Dodatkowo wprowadzono klasę **AlternatingEncoderBlock**, która grupuje bloki kodera w pary. Pierwszy blok w parze działa na standardowych oknach, a drugi na przesuniętych oknach, co pozwala na efektywne uchwycenie relacji między tokenami w różnych lokalizacjach.\n",
        "\n",
        "![image.png](../images/transformer_encoder_block.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "OAzlYRGAuHEe"
      },
      "outputs": [],
      "source": [
        "class SwinEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size, mask, sd_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.stochastic_depth = StochasticDepth(sd_prob, \"row\") # Stochastic Depth with 0.1 probability of dropping out a row for tiny version of Swin Transformer\n",
        "\n",
        "        self.WMSA = ShiftedWindowMSA(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=mask)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim*4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=0.1), # Default dropout probability is 0.0 in the torchvision implementation\n",
        "            nn.Linear(embed_dim*4, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Initialization of weights and biases (bias) in linear layers\n",
        "        for m in self.MLP:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight) # Xavier initialization for weights, which prevents the disappearance or explosion of gradients during training.\n",
        "                if m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, std=1e-6) # Set a small offset, to have a small impact in the initial stages of training.\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Attention path with pre-normalization\n",
        "        res1 = x # Save input for the skip connection\n",
        "        x = self.stochastic_depth(self.WMSA(self.layer_norm(x))) # Attention block with LayerNorm and Stochastic Depth(more efficient than Dropout for training transformers)\n",
        "        x = res1 + x # Residual connection\n",
        "\n",
        "        # MLP path with pre-normalization\n",
        "        res2 = x  # Save intermediate result for skip connection\n",
        "        x = self.stochastic_depth(self.MLP(self.layer_norm(x))) # MLP block with LayerNorm and Dropout\n",
        "        x = res2 + x  # Residual connection\n",
        "\n",
        "        return x\n",
        "\n",
        "class AlternatingEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, sd_prob, window_size=7):\n",
        "        super().__init__()\n",
        "        self.WSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=False, sd_prob=sd_prob[0])\n",
        "        self.SWSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=True, sd_prob=sd_prob[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.SWSA(self.WSA(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRY6oIyNuHEe"
      },
      "source": [
        "# Final Swin-Transformer Class\n",
        "\n",
        "Mając już zaimplementowane wszystkie komponenty Swin-Transformera, możemy stworzyć jego finalną klasę. Struktura modelu opiera się na oryginalnym artykule, uwzględniając odpowiednie bloki kodera, wymiary osadzeń oraz liczbę głów uwagi.\n",
        "\n",
        "Model zaczyna się od warstwy osadzania (*Embedding Layer*), która przekształca obraz wejściowy w odpowiednią reprezentację. Następnie przechodzi przez cztery etapy obliczeniowe:  \n",
        "1. **Etap 1**: Alternating Encoder Block z 96 wymiarami osadzania i 3 głowami uwagi.  \n",
        "2. **Etap 2**: Alternating Encoder Block z 192 wymiarami osadzania i 6 głowami uwagi.  \n",
        "3. **Etap 3**: Trzy następujące po sobie Alternating Encoder Blocks z 384 wymiarami osadzania i 12 głowami uwagi.  \n",
        "4. **Etap 4**: Alternating Encoder Block z 768 wymiarami osadzania i 24 głowami uwagi.  \n",
        "\n",
        "Każdy etap zawiera proces *Patch Merging*, który zmniejsza rozdzielczość przestrzenną danych i zwiększa liczbę wymiarów kanałów. Finalnie, dane wyjściowe mają wymiary `(1, 49, 768)`, gdzie 1 to wymiar partii, 49 to spłaszczona przestrzeń 7x7, a 768 to liczba kanałów reprezentująca wymiar osadzania.\n",
        "\n",
        "Testując model z obrazem wejściowym o wymiarach `(1, 3, 224, 224)`, możemy potwierdzić, że implementacja działa zgodnie z oczekiwaniami i generuje poprawne dane wyjściowe. Dzięki temu w pełni zaimplementowaliśmy Swin-Transformer w PyTorch od podstaw!\n",
        "\n",
        "![image.png](../images/all_stages_swin.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "_CaQahw4uHEe"
      },
      "outputs": [],
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, depth=[2, 2, 6, 2], embed_dim=96, stochastic_depth_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.Embedding = SwinEmbedding()  # Embedding layer\n",
        "\n",
        "        # Calculate total number of blocks\n",
        "        total_stage_blocks = sum(depth)\n",
        "        stage_block_id = 0\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "\n",
        "        in_channels = embed_dim\n",
        "        for i_stage, num_blocks in enumerate(depth):\n",
        "            temp_sd_prob = []\n",
        "            for _ in range(num_blocks):\n",
        "                # Calculate probability for the current layer\n",
        "                sd_prob = stochastic_depth_prob * float(stage_block_id) / (total_stage_blocks - 1)\n",
        "                temp_sd_prob.append(sd_prob)\n",
        "                stage_block_id += 1\n",
        "\n",
        "            #Add alternating encoder blocks recording to the depth list divided by 2, because each block has 2 sub-blocks\n",
        "            sd_prob = [temp_sd_prob[i:i+2] for i in range(0, len(temp_sd_prob), 2)]\n",
        "            for _ in range(int(num_blocks / 2)):\n",
        "                num_heads = in_channels // 32\n",
        "                #print(f\"AlternatingEncoderBlock({in_channels}, {num_heads}, {sd_prob[0]})\") # Debug\n",
        "                self.stages.append(\n",
        "                    AlternatingEncoderBlock(in_channels, num_heads, sd_prob[0])\n",
        "                )\n",
        "                sd_prob.pop(0)\n",
        "\n",
        "            # Add patch merging layer if this is not the last stage\n",
        "            if i_stage < len(depth) - 1:\n",
        "                self.stages.append(PatchMerging(in_channels))\n",
        "                #print(f\"PatchMerging({in_channels})\") # Debug\n",
        "                in_channels *= 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Embedding(x)\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHSJ2HfZuHEe",
        "outputId": "96734324-1d38-4317-db03-e16fc8eef591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 49, 768])\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    x = torch.randn((1,3,224,224)).cuda()\n",
        "    model = SwinTransformer().cuda()\n",
        "    print(model(x).shape)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "\n",
        "class SwinBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    A minimal wrapper that uses your SwinTransformer as a backbone for Mask R-CNN.\n",
        "    Expects input of shape (B,3,H,W). Returns a single feature map dict.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained_swin: nn.Module):\n",
        "        super().__init__()\n",
        "        self.body = pretrained_swin  # your existing SwinTransformer\n",
        "\n",
        "        # You must tell MaskRCNN how many channels the backbone returns.\n",
        "        # In \"tiny\" Swin, the final stage has embedding dim = 768.\n",
        "        self.out_channels = 768\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Backbone input shape:\", x.shape)  # debug\n",
        "\n",
        "        \"\"\"\n",
        "        B, 3, 224, 224 --> B, 49, 768 from your Swin.\n",
        "        We reshape it to (B, 768, 7, 7) so Mask R-CNN sees a spatial feature map.\n",
        "        Then store in an OrderedDict for MaskRCNN.\n",
        "        \"\"\"\n",
        "        # Run Swin\n",
        "        features = self.body(x)  # shape [B, 49, 768]\n",
        "\n",
        "        # Reshape to (B, 7, 7, 768)\n",
        "        B, N, C = features.shape\n",
        "        # For a 224x224 input, after 4 patch merges, we get a 7x7 grid => N=49\n",
        "        # If your input size or stage depth changes, adjust accordingly.\n",
        "        spatial_size = int(N**0.5)  # e.g. 7 for 49 tokens\n",
        "        features = features.view(B, spatial_size, spatial_size, C)\n",
        "\n",
        "        # Permute to (B, C, H, W)\n",
        "        features = features.permute(0, 3, 1, 2).contiguous()  # (B, 768, 7, 7)\n",
        "\n",
        "        # Return an OrderedDict\n",
        "        out = OrderedDict()\n",
        "        out[\"out\"] = features\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "tBvMnNZgxOYd"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "\n",
        "def get_mask_rcnn_swin_model(num_classes=2, pretrained_swin=None):\n",
        "    if pretrained_swin is None:\n",
        "        pretrained_swin = SwinTransformer()\n",
        "\n",
        "    backbone = SwinBackbone(pretrained_swin)\n",
        "    backbone.out_channels = 768\n",
        "\n",
        "    anchor_generator = AnchorGenerator(\n",
        "      sizes=((32,), (64,), (128,), (256,)),\n",
        "      aspect_ratios=((0.5,1.0,2.0),)*4\n",
        "  )\n",
        "\n",
        "    # Build a custom transform that does NOT upsize to 800\n",
        "    # but rather keeps 224 or the image’s original size.\n",
        "    transform = GeneralizedRCNNTransform(\n",
        "        min_size=224,   # or (224,) if you want a tuple\n",
        "        max_size=224,   # lock both min and max to 224\n",
        "        image_mean=[0.0, 0.0, 0.0],  # or your dataset's mean\n",
        "        image_std=[1.0, 1.0, 1.0],   # or your dataset's std\n",
        "    )\n",
        "\n",
        "    model = MaskRCNN(\n",
        "        backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_detections_per_img=100,\n",
        "        image_mean=None,  # We’ll rely on our custom transform\n",
        "        image_std=None,\n",
        "        transform=transform,  # use our custom transform\n",
        "    )\n",
        "    model.transform = transform\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "H6Qe3BLNxQJf"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "class SwinTransformerMultiStage(nn.Module):\n",
        "    \"\"\"\n",
        "    Subclass (or replacement) of your SwinTransformer that returns\n",
        "    4 feature maps from each stage: C2, C3, C4, C5.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_swin):\n",
        "        super().__init__()\n",
        "        # Copy over the embedding\n",
        "        self.Embedding = base_swin.Embedding\n",
        "        # Copy over the entire 'stages' ModuleList\n",
        "        self.stages = base_swin.stages\n",
        "        # You already know embed_dim=96 for tiny model, but not strictly needed here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1) Patch embedding\n",
        "        x = self.Embedding(x)  # (B, 56*56, 96)\n",
        "\n",
        "        # -- Stage 1\n",
        "        x = self.stages[0](x)\n",
        "        c2 = x\n",
        "        x = self.stages[1](x)\n",
        "\n",
        "        # -- Stage 2\n",
        "        x = self.stages[2](x)\n",
        "        c3 = x\n",
        "        x = self.stages[3](x)\n",
        "\n",
        "        # -- Stage 3\n",
        "        x = self.stages[4](x)\n",
        "        x = self.stages[5](x)\n",
        "        x = self.stages[6](x)\n",
        "        c4 = x\n",
        "        x = self.stages[7](x)\n",
        "\n",
        "        # -- Stage 4\n",
        "        x = self.stages[8](x)\n",
        "        c5 = x\n",
        "\n",
        "        # Now unflatten each stage\n",
        "        # c2 -> (B, 3136, 96) => (B, 96, 56, 56)\n",
        "        # c3 -> (B, 784, 192) => (B, 192, 28, 28)\n",
        "        # c4 -> (B, 196, 384) => (B, 384, 14, 14)\n",
        "        # c5 -> (B, 49, 768) => (B, 768, 7, 7)\n",
        "\n",
        "        c2 = unflatten_and_transpose(c2, 96, 56, 56)\n",
        "        c3 = unflatten_and_transpose(c3, 192, 28, 28)\n",
        "        c4 = unflatten_and_transpose(c4, 384, 14, 14)\n",
        "        c5 = unflatten_and_transpose(c5, 768, 7, 7)\n",
        "\n",
        "        return {\"c2\": c2, \"c3\": c3, \"c4\": c4, \"c5\": c5}\n"
      ],
      "metadata": {
        "id": "daBqHcxj32s3"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unflatten_and_transpose(x, c, h, w):\n",
        "    \"\"\"\n",
        "    x: (B, h*w, c)\n",
        "    want: (B, c, h, w)\n",
        "    \"\"\"\n",
        "    B = x.shape[0]\n",
        "    return (\n",
        "        x.view(B, h, w, c)  # (B, H, W, C)\n",
        "         .permute(0, 3, 1, 2)  # (B, C, H, W)\n",
        "         .contiguous()\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Hr-1-VIz36wb"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision.ops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKICNmsR4lxQ",
        "outputId": "be0e69ae-45c9-417c-9c24-01ab6e388c06"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision.ops (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision.ops\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.ops import FeaturePyramidNetwork\n",
        "\n",
        "class SwinFPNBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    1) Runs the Swin stages -> returns c2..c5\n",
        "    2) Feeds them into a standard FeaturePyramidNetwork -> returns multi-scale feature maps\n",
        "    3) That final dict is what Mask R-CNN expects\n",
        "    \"\"\"\n",
        "    def __init__(self, swin_multistage: nn.Module):\n",
        "        super().__init__()\n",
        "        self.swin = swin_multistage\n",
        "        # Suppose we output 256 channels from FPN\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=[96, 192, 384, 768],  # channels in c2..c5\n",
        "            out_channels=256,\n",
        "            # extra_blocks=LastLevelMaxPool()  # optional\n",
        "        )\n",
        "        self.out_channels = 256  # FPN’s output channels per scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is (B,3,H,W)\n",
        "        # 1) Get raw stage features\n",
        "        features = self.swin(x)  # e.g. {\"c2\":(B,96,56,56), \"c3\":(B,192,28,28), \"c4\":(B,384,14,14), \"c5\":(B,768,7,7)}\n",
        "\n",
        "        # 2) Rename them to match FPN’s expected keys: \"0\", \"1\", \"2\", \"3\" or something\n",
        "        #    or you can pass them in as a dict with the same keys but then set in_channels_list accordingly\n",
        "        fpn_input = {\n",
        "        \"0\": features[\"c2\"],\n",
        "        \"1\": features[\"c3\"],\n",
        "        \"2\": features[\"c4\"],\n",
        "        \"3\": features[\"c5\"],\n",
        "         }\n",
        "\n",
        "        # 3) Run FPN\n",
        "        #    This returns a dict of feature maps at different scales (e.g. \"res2\", \"res3\", \"res4\", \"res5\")\n",
        "        #    each will have shape (B, 256, H_out, W_out)\n",
        "        out = self.fpn(fpn_input)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "dUQMM_Ar4aE5"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "\n",
        "def build_swin_maskrcnn(num_classes=2):\n",
        "    base_swin = SwinTransformer()  # your existing code\n",
        "    # Convert it to multi-stage\n",
        "    multi_stage_swin = SwinTransformerMultiStage(base_swin)\n",
        "    # Wrap in FPN\n",
        "    backbone = SwinFPNBackbone(multi_stage_swin)\n",
        "\n",
        "    # For multi-scale anchors\n",
        "    anchor_generator = anchor_generator = AnchorGenerator(\n",
        "    sizes=((32,), (64,), (128,), (256,)),  # 4 \"levels\"\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)*4     # or explicitly write 4 tuples\n",
        ")\n",
        "\n",
        "\n",
        "    transform = GeneralizedRCNNTransform(\n",
        "        min_size=224,\n",
        "        max_size=224,\n",
        "        image_mean=[0.0, 0.0, 0.0],\n",
        "        image_std=[1.0, 1.0, 1.0],\n",
        "    )\n",
        "\n",
        "    model = MaskRCNN(\n",
        "        backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_detections_per_img=100,\n",
        "        image_mean=None,\n",
        "        image_std=None,\n",
        "        transform=transform\n",
        "    )\n",
        "    # Force it in case older torchvision\n",
        "    model.transform = transform\n",
        "    return model"
      ],
      "metadata": {
        "id": "OreAmIUM4EhC"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = build_swin_maskrcnn(num_classes=2).to(device)\n",
        "    x = [torch.randn(3, 224, 224, device=device)]\n",
        "    targets = [{\n",
        "        \"boxes\": torch.tensor([[50,50,150,150]], dtype=torch.float32, device=device),\n",
        "        \"labels\": torch.tensor([1], device=device),\n",
        "        \"masks\": torch.randint(0,2,(1,224,224), device=device, dtype=torch.uint8),\n",
        "    }]\n",
        "\n",
        "    model.train()\n",
        "    losses = model(x, targets)  # forward pass -> dict of losses\n",
        "    print(losses)  # e.g. { 'loss_classifier':..., 'loss_box_reg':..., ... }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(x)  # inference\n",
        "        print(preds)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ9x9vOl4RIO",
        "outputId": "9da64df5-aa3b-4aaf-e82b-593f50d7d615"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss_classifier': tensor(0.7854, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>), 'loss_mask': tensor(8.0395, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.6732, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0068, device='cuda:0', grad_fn=<DivBackward0>)}\n",
            "[{'boxes': tensor([[0.0000e+00, 1.9895e+02, 4.4365e+01, 2.1841e+02],\n",
            "        [1.6208e+02, 1.9276e+02, 2.0805e+02, 2.1489e+02],\n",
            "        [6.3835e+01, 1.1064e+02, 8.9819e+01, 1.4702e+02],\n",
            "        [1.9472e+02, 1.0431e+02, 2.2374e+02, 2.0606e+02],\n",
            "        [1.0626e+02, 1.7371e+02, 1.5521e+02, 1.9602e+02],\n",
            "        [1.3216e+02, 8.0594e+01, 1.7401e+02, 1.0331e+02],\n",
            "        [9.9049e+01, 4.4878e+01, 1.4722e+02, 6.5660e+01],\n",
            "        [2.7125e-01, 1.0882e+02, 1.2065e+01, 1.5456e+02],\n",
            "        [1.4383e+02, 1.9172e+02, 1.8589e+02, 2.1494e+02],\n",
            "        [5.0383e+01, 2.2511e-01, 1.0082e+02, 1.0683e+01],\n",
            "        [0.0000e+00, 5.5881e+01, 3.0605e+01, 8.1190e+01],\n",
            "        [1.7083e+02, 1.5373e+02, 2.1069e+02, 1.7484e+02],\n",
            "        [3.7805e+01, 8.0080e+01, 9.3993e+01, 1.5235e+02],\n",
            "        [0.0000e+00, 2.4482e+01, 4.1080e+01, 1.3411e+02],\n",
            "        [4.6721e+01, 1.6904e+02, 9.4413e+01, 1.9278e+02],\n",
            "        [1.5665e+02, 8.1200e+01, 1.9491e+02, 1.7756e+02],\n",
            "        [1.8779e+02, 1.3900e-01, 2.2302e+02, 1.1496e+01],\n",
            "        [2.4345e-01, 3.3077e+01, 2.3631e+01, 5.5655e+01],\n",
            "        [6.7077e+01, 8.7815e+01, 1.0849e+02, 1.8199e+02],\n",
            "        [0.0000e+00, 4.7833e+01, 2.7522e+01, 6.9899e+01],\n",
            "        [1.4811e+02, 0.0000e+00, 2.2400e+02, 9.1023e+01],\n",
            "        [1.0239e+02, 7.9596e+01, 1.6267e+02, 1.0205e+02],\n",
            "        [3.5794e+01, 1.1994e+02, 6.1475e+01, 1.7218e+02],\n",
            "        [1.3359e+02, 1.5583e+02, 1.7852e+02, 2.2367e+02],\n",
            "        [1.6244e+02, 1.7530e+02, 1.9079e+02, 2.0844e+02],\n",
            "        [1.9036e+02, 7.7025e+01, 2.2354e+02, 9.7177e+01],\n",
            "        [8.5676e+01, 1.6793e+02, 1.3547e+02, 1.8949e+02],\n",
            "        [2.4566e+01, 7.0541e+01, 6.4757e+01, 1.6727e+02],\n",
            "        [1.4537e+02, 1.7890e+02, 2.0022e+02, 1.9954e+02],\n",
            "        [0.0000e+00, 2.3281e+01, 1.2106e+01, 1.1612e+02],\n",
            "        [8.0163e+01, 1.3042e+02, 1.2190e+02, 2.2400e+02],\n",
            "        [8.6027e+01, 3.2113e+01, 1.6741e+02, 8.3369e+01],\n",
            "        [1.0289e+02, 1.0399e+02, 1.5907e+02, 1.2602e+02],\n",
            "        [1.6410e+01, 2.3997e+00, 6.7659e+01, 6.0039e+01],\n",
            "        [0.0000e+00, 1.8042e+02, 3.9253e+01, 2.2400e+02],\n",
            "        [7.5634e+01, 5.1214e+01, 1.1765e+02, 1.5267e+02],\n",
            "        [5.0807e+01, 1.5160e+02, 9.8864e+01, 2.2400e+02],\n",
            "        [1.5060e+01, 1.5095e+02, 3.8119e+01, 1.8148e+02],\n",
            "        [1.1539e+01, 6.8095e+01, 5.2242e+01, 1.7193e+02],\n",
            "        [0.0000e+00, 9.5910e+01, 3.7440e+01, 1.1720e+02],\n",
            "        [1.3123e+02, 6.2493e+01, 1.7913e+02, 8.2860e+01],\n",
            "        [0.0000e+00, 1.8796e+02, 4.7141e+01, 2.1130e+02],\n",
            "        [1.6194e+00, 7.3895e+01, 4.8713e+01, 9.7245e+01],\n",
            "        [2.1161e+01, 4.2528e-01, 1.1000e+02, 3.8145e+01],\n",
            "        [1.2650e+02, 1.1051e+02, 1.5002e+02, 1.3752e+02],\n",
            "        [2.1332e+01, 3.4788e+00, 4.4958e+01, 3.2779e+01],\n",
            "        [1.0138e+02, 5.4697e+01, 1.5320e+02, 7.6287e+01],\n",
            "        [1.8424e+02, 2.8651e+01, 2.2400e+02, 5.0379e+01],\n",
            "        [4.2195e+01, 6.8297e+01, 1.0545e+02, 1.2776e+02],\n",
            "        [1.6302e+02, 1.0479e+00, 1.8537e+02, 3.4816e+01],\n",
            "        [1.0815e+02, 6.0900e+01, 1.5129e+02, 1.6017e+02],\n",
            "        [1.9371e+01, 1.0242e+02, 1.3092e+02, 2.0667e+02],\n",
            "        [5.0949e+01, 7.8720e+00, 1.0063e+02, 2.8998e+01],\n",
            "        [1.0490e+02, 1.5187e-01, 1.5319e+02, 1.2366e+01],\n",
            "        [1.4832e+02, 2.5617e+00, 1.8663e+02, 2.2853e+01],\n",
            "        [1.2402e+02, 6.6558e+01, 1.7604e+02, 9.2467e+01],\n",
            "        [1.9595e+02, 4.8110e+00, 2.2394e+02, 2.6212e+01],\n",
            "        [1.8652e+02, 1.6624e+02, 2.2400e+02, 1.9008e+02],\n",
            "        [8.6354e-02, 2.4379e+00, 1.8466e+01, 8.0952e+01],\n",
            "        [0.0000e+00, 1.1960e+02, 1.7953e+01, 2.1188e+02],\n",
            "        [1.4927e+01, 1.6966e+01, 5.9212e+01, 3.6333e+01],\n",
            "        [3.0374e+01, 1.3280e+02, 7.0156e+01, 1.5518e+02],\n",
            "        [1.4874e+02, 3.1334e+00, 1.7731e+02, 3.5797e+01],\n",
            "        [2.6943e+01, 1.2743e+02, 7.6307e+01, 1.4529e+02],\n",
            "        [6.7314e+01, 1.0698e+02, 1.1572e+02, 1.3077e+02],\n",
            "        [3.0339e+01, 1.1535e+02, 8.0931e+01, 1.3826e+02],\n",
            "        [1.1914e+02, 1.1674e+02, 1.6548e+02, 1.3854e+02],\n",
            "        [0.0000e+00, 9.1698e+01, 1.7927e+01, 1.2423e+02],\n",
            "        [1.2210e+02, 3.1102e+00, 1.4772e+02, 2.9818e+01],\n",
            "        [1.4521e+02, 1.2974e+02, 1.6911e+02, 1.6049e+02],\n",
            "        [1.3606e+02, 1.1570e+02, 1.5828e+02, 1.4101e+02],\n",
            "        [4.0825e+01, 9.2081e+01, 8.1482e+01, 1.1515e+02],\n",
            "        [1.4579e+02, 1.9937e+01, 1.9111e+02, 4.2347e+01],\n",
            "        [1.3146e+02, 9.9028e+01, 1.8090e+02, 1.2559e+02],\n",
            "        [0.0000e+00, 9.2132e+01, 9.0237e+01, 1.4535e+02],\n",
            "        [1.6752e+02, 1.2748e+01, 2.2352e+02, 6.8876e+01],\n",
            "        [1.3472e+02, 1.6227e+02, 1.5669e+02, 2.1390e+02],\n",
            "        [3.2772e-01, 8.5571e+01, 1.5835e+01, 1.8346e+02],\n",
            "        [1.4845e+02, 1.8621e+02, 1.9958e+02, 2.0790e+02],\n",
            "        [5.3301e-01, 1.6272e+02, 2.1061e+01, 2.2400e+02],\n",
            "        [1.3205e+02, 1.3310e+02, 1.7931e+02, 1.5240e+02],\n",
            "        [0.0000e+00, 5.4773e+01, 3.9855e+01, 1.0038e+02],\n",
            "        [6.8066e+01, 1.1796e+02, 1.1557e+02, 1.4044e+02],\n",
            "        [1.2866e+02, 1.5243e+01, 1.7373e+02, 3.6231e+01],\n",
            "        [1.5631e+02, 5.5051e+01, 2.0105e+02, 7.5476e+01],\n",
            "        [1.0039e+02, 3.6705e+01, 1.3060e+02, 6.5237e+01],\n",
            "        [1.9387e+02, 5.1711e+01, 2.1144e+02, 9.4963e+01],\n",
            "        [1.1186e+02, 1.9311e+01, 1.3423e+02, 4.6421e+01],\n",
            "        [1.1015e+02, 1.3304e+02, 1.5887e+02, 1.5267e+02],\n",
            "        [3.1524e+01, 4.9024e-01, 5.4620e+01, 2.4077e+01],\n",
            "        [1.9917e+00, 1.3607e+01, 8.1032e+01, 6.8168e+01],\n",
            "        [1.9257e+02, 1.5441e+02, 2.2400e+02, 1.7878e+02],\n",
            "        [1.3559e+02, 1.8046e+02, 1.8110e+02, 2.0305e+02],\n",
            "        [7.9371e+01, 1.8310e+02, 1.0070e+02, 2.2400e+02],\n",
            "        [6.5195e+01, 2.1929e+01, 1.7527e+02, 1.1848e+02],\n",
            "        [1.4465e+02, 5.8614e-01, 1.9454e+02, 7.7044e+01],\n",
            "        [2.9858e+01, 1.5737e+02, 5.8517e+01, 1.9014e+02],\n",
            "        [1.6790e+02, 2.9812e+01, 2.1551e+02, 5.3683e+01],\n",
            "        [9.4830e+01, 1.0656e+02, 1.4303e+02, 1.3230e+02],\n",
            "        [1.2147e+02, 7.4344e+01, 1.6566e+02, 9.7469e+01]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.6816, 0.6732, 0.6618, 0.6584, 0.6568, 0.6567, 0.6538, 0.6538, 0.6533,\n",
            "        0.6477, 0.6465, 0.6457, 0.6454, 0.6444, 0.6400, 0.6392, 0.6386, 0.6383,\n",
            "        0.6374, 0.6369, 0.6359, 0.6354, 0.6348, 0.6341, 0.6338, 0.6336, 0.6335,\n",
            "        0.6332, 0.6327, 0.6309, 0.6309, 0.6308, 0.6301, 0.6298, 0.6290, 0.6289,\n",
            "        0.6286, 0.6263, 0.6258, 0.6252, 0.6246, 0.6234, 0.6232, 0.6230, 0.6227,\n",
            "        0.6223, 0.6207, 0.6207, 0.6203, 0.6202, 0.6196, 0.6195, 0.6192, 0.6190,\n",
            "        0.6189, 0.6181, 0.6175, 0.6174, 0.6172, 0.6171, 0.6170, 0.6169, 0.6167,\n",
            "        0.6165, 0.6157, 0.6152, 0.6151, 0.6145, 0.6145, 0.6144, 0.6143, 0.6140,\n",
            "        0.6135, 0.6133, 0.6127, 0.6124, 0.6122, 0.6119, 0.6117, 0.6116, 0.6114,\n",
            "        0.6113, 0.6112, 0.6111, 0.6110, 0.6108, 0.6107, 0.6103, 0.6102, 0.6100,\n",
            "        0.6097, 0.6097, 0.6095, 0.6094, 0.6091, 0.6089, 0.6079, 0.6077, 0.6075,\n",
            "        0.6072], device='cuda:0'), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}