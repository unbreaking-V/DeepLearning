{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:** [tutorial](https://www.kaggle.com/code/armanasgharpoor1993/coco-dataset-tutorial-image-segmentation#Step-16:-Generating-Image-and-Mask-Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie generatora do wsadowego generowania wstępnie przetworzonych obrazów i masek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_path, masks_path, annotation_dir, image_size=224, batch_size=1):\n",
    "        \"\"\"\n",
    "        CocoDataset class for generating batches of images, masks, bounding boxes, and labels.\n",
    "\n",
    "        Args:\n",
    "            images_path (str): Path to the directory containing the original images.\n",
    "            masks_path (str): Path to the directory containing the masks.\n",
    "            annotation_dir (object): COCO-like annotations object containing bounding boxes and labels.\n",
    "            image_size (int): Target size for resizing images and masks (default: 224).\n",
    "            batch_size (int): Number of samples in each batch (default: 1).\n",
    "        \"\"\"\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.coco = annotation_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.image_filenames = self.get_matching_filenames()\n",
    "        self.cat_ids = self.coco.getCatIds()\n",
    "\n",
    "    def get_matching_filenames(self):\n",
    "        \"\"\"\n",
    "        Get the list of matching filenames between images and masks.\n",
    "\n",
    "        Returns:\n",
    "            list: List of matching filenames.\n",
    "        \"\"\"\n",
    "        image_files = set([os.path.splitext(filename)[0] for filename in os.listdir(self.images_path)])\n",
    "        mask_files = set([os.path.splitext(filename)[0] for filename in os.listdir(self.masks_path)])\n",
    "        matching_files = list(image_files.intersection(mask_files))\n",
    "        return matching_files\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of batches in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of batches.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a batch of preprocessed samples of images, masks, bounding boxes, and labels.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Batch index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing batch of images, masks, bounding boxes, and labels.\n",
    "        \"\"\"\n",
    "        batch_start = idx * self.batch_size\n",
    "        batch_end = min((idx + 1) * self.batch_size, len(self.image_filenames))\n",
    "        batch_filenames = self.image_filenames[batch_start:batch_end]\n",
    "\n",
    "        images = []\n",
    "        masks = []\n",
    "        bounding_boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for filename in batch_filenames:\n",
    "            # Load image and mask\n",
    "            image_path = os.path.join(self.images_path, filename + '.jpg')\n",
    "            mask_path = os.path.join(self.masks_path, filename + '.jpg')\n",
    "\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            mask = Image.open(mask_path)\n",
    "\n",
    "            # Save original dimensions for scaling bounding boxes\n",
    "            original_width, original_height = image.size\n",
    "\n",
    "            # Resize image and mask to target size\n",
    "            image = image.resize((self.image_size, self.image_size))\n",
    "            mask = mask.resize((self.image_size, self.image_size))\n",
    "\n",
    "            # Convert image and mask to numpy arrays\n",
    "            image = np.array(image) / 255.0  # Normalize image\n",
    "            mask = np.array(mask)  # Binary or multi-class mask\n",
    "\n",
    "            # Retrieve annotations for the image\n",
    "            img_id = self.coco.getImgIds()[self.image_filenames.index(filename)]\n",
    "\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.cat_ids, iscrowd=None)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            img_bboxes = []\n",
    "            img_labels = []\n",
    "\n",
    "            # Process annotations (scale bounding boxes to new image size)\n",
    "            for ann in anns:\n",
    "                bbox = ann['bbox']\n",
    "                category_id = ann['category_id']\n",
    "\n",
    "                # Scale bounding box to resized image dimensions\n",
    "                x, y, width, height = bbox\n",
    "                x = x * (self.image_size / original_width)\n",
    "                y = y * (self.image_size / original_height)\n",
    "                width = width * (self.image_size / original_width)\n",
    "                height = height * (self.image_size / original_height)\n",
    "\n",
    "                img_bboxes.append([x, y, x + width, y + height])\n",
    "                img_labels.append(category_id)\n",
    "\n",
    "            # Append data for this image to the batch\n",
    "            images.append(image.transpose(2, 0, 1))  # Convert to channels-first format\n",
    "            masks.append(mask)\n",
    "            bounding_boxes.append(img_bboxes)\n",
    "            labels.append(img_labels)\n",
    "\n",
    "        # Convert all batch data to tensors\n",
    "        images = torch.tensor(images, dtype=torch.float32)\n",
    "        masks = torch.tensor(masks, dtype=torch.float32).unsqueeze(1)  # Add channel dimension for masks\n",
    "        bounding_boxes = [torch.tensor(bboxes, dtype=torch.float32) for bboxes in bounding_boxes]\n",
    "        labels = [torch.tensor(lbls, dtype=torch.int64) for lbls in labels]\n",
    "\n",
    "        return {\n",
    "            \"images\": images,  # Shape: (batch_size, 3, 224, 224)\n",
    "            \"masks\": masks,  # Shape: (batch_size, 1, 224, 224)\n",
    "            \"bounding_boxes\": bounding_boxes,  # List of tensors (1 per image)\n",
    "            \"labels\": labels  # List of tensors (1 per image)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Images shape: torch.Size([8, 3, 224, 224])\n",
      "Masks shape: torch.Size([8, 1, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1635510/3520591583.py:110: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  images = torch.tensor(images, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "images_path = 'coco10/train2017_subset/images'\n",
    "masks_path = 'coco10/train2017_subset/masks'\n",
    "annotation_dir = \"coco10/train2017_subset/coco10_train_annotations.json\"\n",
    "batch_size = 8\n",
    "\n",
    "# Initialize the data generator\n",
    "train_data_generator = CocoDataset(\n",
    "    images_path=images_path,\n",
    "    masks_path=masks_path,\n",
    "    annotation_dir = COCO(annotation_dir),\n",
    "    image_size=224,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Fetch a batch of data\n",
    "batch = train_data_generator[0]\n",
    "\n",
    "# Inspect shapes of images and masks\n",
    "print(f\"Images shape: {batch['images'].shape}\")  # (batch_size, 3, 224, 224)\n",
    "print(f\"Masks shape: {batch['masks'].shape}\")    # (batch_size, 1, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 2\n",
      "  Bounding boxes: [[1.0149999856948853, 0.7886666655540466, 128.1595001220703, 197.81533813476562], [110.9990005493164, 0.0, 223.68850708007812, 199.73333740234375]]\n",
      "  Labels: [24, 24]\n",
      "Sample 1:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[1.8976943492889404, 0.9240000247955322, 288.7558288574219, 147.07000732421875]]\n",
      "  Labels: [23]\n",
      "Sample 2:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 6\n",
      "  Bounding boxes: [[44.432498931884766, 67.0373764038086, 92.8375015258789, 168.77639770507812], [88.99800109863281, 66.0144271850586, 160.3314971923828, 126.09574127197266], [140.6439971923828, 28.98885154724121, 179.89300537109375, 79.40196990966797], [159.58250427246094, 60.93639373779297, 211.20399475097656, 146.3973846435547], [101.35299682617188, 115.56196594238281, 167.62550354003906, 147.65115356445312], [89.42500305175781, 109.63410186767578, 108.66449737548828, 136.14688110351562]]\n",
      "  Labels: [22, 22, 22, 22, 22, 22]\n",
      "Sample 3:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 4\n",
      "  Bounding boxes: [[134.70449829101562, 85.479248046875, 175.58799743652344, 215.46792602539062], [20.149499893188477, 83.64603424072266, 96.3864974975586, 222.0347137451172], [116.7074966430664, 41.26565933227539, 155.47349548339844, 145.89584350585938], [54.94649887084961, 121.99018859863281, 67.79850006103516, 136.03773498535156]]\n",
      "  Labels: [25, 25, 25, 25]\n",
      "Sample 4:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[44.974998474121094, 57.138668060302734, 130.68299865722656, 110.31999969482422]]\n",
      "  Labels: [17]\n",
      "Sample 5:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[89.93599700927734, 71.7158432006836, 272.2854309082031, 249.9242706298828]]\n",
      "  Labels: [17]\n",
      "Sample 6:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[30.446500778198242, 32.7599983215332, 131.16949462890625, 158.75067138671875]]\n",
      "  Labels: [16]\n",
      "Sample 7:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[94.75900268554688, 8.611555099487305, 147.99049377441406, 292.51287841796875]]\n",
      "  Labels: [25]\n",
      "Sample 8:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 3\n",
      "  Bounding boxes: [[44.89799880981445, 104.84600067138672, 114.25399780273438, 182.1353302001953], [96.49500274658203, 8.040666580200195, 178.9199981689453, 171.99465942382812], [68.21499633789062, 148.6006622314453, 81.25599670410156, 172.9046630859375]]\n",
      "  Labels: [24, 25, 24]\n",
      "Sample 9:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[75.29900360107422, 46.22163772583008, 119.18900299072266, 190.8354034423828]]\n",
      "  Labels: [23]\n",
      "Sample 10:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[160.08999633789062, 38.615501403808594, 177.2726593017578, 63.97999954223633]]\n",
      "  Labels: [18]\n",
      "Sample 11:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 2\n",
      "  Bounding boxes: [[107.33450317382812, 51.04871368408203, 165.21400451660156, 184.6336669921875], [11.525500297546387, 50.20594024658203, 94.54900360107422, 184.6336669921875]]\n",
      "  Labels: [19, 19]\n",
      "Sample 12:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[10.741205215454102, 67.5111083984375, 107.27710723876953, 144.4938201904297]]\n",
      "  Labels: [16]\n",
      "Sample 13:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[29.86549949645996, 149.7894287109375, 73.22000122070312, 282.85284423828125]]\n",
      "  Labels: [16]\n",
      "Sample 14:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 2\n",
      "  Bounding boxes: [[1.5470000505447388, 117.91018676757812, 9.943499565124512, 141.85403442382812], [59.856998443603516, 102.59484100341797, 89.82749938964844, 148.0388946533203]]\n",
      "  Labels: [25, 25]\n",
      "Sample 15:\n",
      "  Shape of preprocessed image: torch.Size([3, 224, 224])\n",
      "  Shape of preprocessed mask: torch.Size([1, 224, 224])\n",
      "  Number of bounding boxes: 1\n",
      "  Bounding boxes: [[7.732480049133301, 58.4192008972168, 186.20672607421875, 300.6796875]]\n",
      "  Labels: [23]\n"
     ]
    }
   ],
   "source": [
    "def validate_image_shapes(generator):\n",
    "    \"\"\"\n",
    "    Print the shapes of preprocessed images, masks, bounding boxes, and labels from the provided generator.\n",
    "\n",
    "    Args:\n",
    "        generator (CustomDataGenerator): Instance of the CustomDataGenerator class.\n",
    "    \"\"\"\n",
    "    for i in range(len(generator)):\n",
    "        # Get a batch of preprocessed samples from the generator\n",
    "        batch = generator[i]  # batch is a dictionary\n",
    "        \n",
    "        images = batch['images']\n",
    "        masks = batch['masks']\n",
    "        bounding_boxes = batch['bounding_boxes']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Print shapes for each item in the batch\n",
    "        for j in range(len(images)):\n",
    "            image = images[j]\n",
    "            mask = masks[j]\n",
    "            bbox = bounding_boxes[j]\n",
    "            lbl = labels[j]\n",
    "\n",
    "            print(f\"Sample {i * generator.batch_size + j}:\")\n",
    "            print(f\"  Shape of preprocessed image: {image.shape}\")\n",
    "            print(f\"  Shape of preprocessed mask: {mask.shape}\")\n",
    "            print(f\"  Number of bounding boxes: {len(bbox)}\")\n",
    "            print(f\"  Bounding boxes: {bbox.tolist()}\")\n",
    "            print(f\"  Labels: {lbl.tolist()}\")\n",
    "        \n",
    "        # Break after a few batches for brevity (optional)\n",
    "        if i >= 1:  # Check only the first two batches\n",
    "            break\n",
    "\n",
    "# Call the validation function\n",
    "validate_image_shapes(train_data_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
